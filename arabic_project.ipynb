{
 "cells": [
  {
   "cell_type": "raw",
   "id": "27a7cdc7-bfc5-4c41-a365-fc003bd639ef",
   "metadata": {},
   "source": [
    "In this notebook we will make Arabic models [video to text - text to video] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33a845f-7d2a-425c-a548-434a130d1255",
   "metadata": {},
   "source": [
    "# 1- Determine the words that we will work on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "768ea82c-591d-4367-991e-a81500682ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import os\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11aad41d-06d2-4953-899d-2e8c32b167e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['أ', 'أب', 'أبن_أو_ولد', 'أبيض', 'أثنين', 'أحبك', 'أحمر', 'أخضر', 'أربعة', 'أسمك', 'أشارة', 'أصم', 'أنا', 'أنت', 'أين', 'الأثنين', 'الأحد', 'الأربعاء', 'الأقصر', 'الاسماعيلية', 'الثلاثاء', 'الحمد لله', 'الخميس', 'السبت', 'السلام', 'الشرقية', 'الصلاة', 'القاهرة', 'النوم_او_ينام', 'ب', 'بتشتغل', 'برتقال', 'بنت_او_ابنة', 'بنك', 'ت', 'تسعة', 'تسكن_او_ساكن', 'تيليفيزيون', 'تين', 'ث', 'ثلاثة', 'ثمانية', 'ج', 'جائع', 'جامعة', 'جيد', 'ح', 'حمام', 'خ', 'خطوبة', 'خمسة', 'د', 'دكتور', 'ذ', 'ر', 'ز', 'س', 'سبعة', 'ستة', 'سرير', 'سعيد', 'سىء', 'ش', 'ص', 'ض', 'ط', 'طاولة', 'طبق', 'طفل', 'طلاق_او_مطلق_او_مطلقة', 'ظ', 'ع', 'عشرة', 'عندك', 'غ', 'ف', 'ق', 'ك', 'كرسي', 'كلية', 'كوب', 'ل', 'م', 'ماء', 'ماذا', 'متزوج', 'مدرسة', 'مساعدة', 'مستشفى', 'مصر', 'معلم', 'منزل_او_بيت', 'مهندس', 'ن', 'نعم', 'ه', 'و', 'واحد', 'ي']\n"
     ]
    }
   ],
   "source": [
    "class_names = []\n",
    "for class_name in os.listdir(\"arabic\"):\n",
    "    class_names.append(class_name.split(\".\")[0])\n",
    "print(class_names) # 90 words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820e2db0-8ba2-435b-9c55-d50c320f6b04",
   "metadata": {},
   "source": [
    "## Create folder for each class in english data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a7c43b3-7cff-4898-b3eb-2d459ca35c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_name in os.listdir(\"arabic\"):\n",
    "    os.mkdir(os.path.join(\"arabic_data\", class_name.split(\".\")[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0eec737-52bc-42bc-b65d-a53d9869fa53",
   "metadata": {},
   "source": [
    "# 2- Collect images for each class [word] -> using web camera\n",
    "- we will collect 2000 images for each class and put them in folders created in english data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d20f95c2-4b48-4448-93fb-b4f3d6e162c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create objects just focus on the hands \n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "# get the model that detect hand_landmarks \n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e4e0a219-eeb9-4ad1-ae2a-b8651bc798a5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setup my camera [my camera has the index (0)]\n",
    "my_camera = cv2.VideoCapture(0)\n",
    "\n",
    "# define a counter to give each frame a unique name \n",
    "counter = 0\n",
    "\n",
    "# In this loop, Frames will be taken from the camera until we stop it.\n",
    "while my_camera.isOpened():\n",
    "    \n",
    "    # read frame by frame from the camera -> return [frame, status of the reading process (is the camera capture the frame right or not)]\n",
    "    # status -> Boolean, Frame --> image with np.array data type\n",
    "    status, frame = my_camera.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    frame_copy = np.copy(frame)\n",
    "    \n",
    "    # convert frame to rgb\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # get hands detection on the rgb image \n",
    "    result = hands.process(frame_rgb)\n",
    "    \n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmark in result.multi_hand_landmarks:\n",
    "            # draw the landmarks\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame_copy,\n",
    "                hand_landmark, \n",
    "                mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                mp_drawing_styles.get_default_hand_connections_style())\n",
    "    \n",
    "    # display the captured frame\n",
    "    cv2.imshow('Captured Frame', frame_copy)\n",
    "\n",
    "    # save the captured frame on pressing [s]\n",
    "    # if cv2.waitKey(1) & 0xFF == ord('s'):\n",
    "    #     # increase counter by 1 \n",
    "    counter += 1 \n",
    "    cv2.imwrite(os.path.join(\"arabic_data\", \"pray\", f\"new_right_pray_frame_{counter}.jpg\"), frame)\n",
    "    # print(f'{counter}_image successfully saved')\n",
    "    \n",
    "    \n",
    "    # Stop the reading process on pressing [q]\n",
    "    if (cv2.waitKey(1) & 0xFF == ord('q')) or counter == 2000:\n",
    "        break\n",
    "    \n",
    "my_camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed34e187-54a0-4c26-81ba-a7e5d16ee9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bcd58a-48d6-4c92-abe7-db3d0f8d7c66",
   "metadata": {},
   "source": [
    "## Remove images that can't be detected by the google model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "35237f46-1acd-4a27-a021-2b2946edbe98",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in os.listdir(os.path.join(\"arabic_data\", \"pray\")):\n",
    "    image = cv2.imread(os.path.join(\"arabic_data\", \"pray\", i))\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(image_rgb)\n",
    "    \n",
    "    if result.multi_hand_landmarks:\n",
    "        continue\n",
    "    else:\n",
    "        print(i)\n",
    "        os.remove(os.path.join(\"arabic_data\", \"pray\", i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98547dba-e604-41e8-9af5-d9f056d1a2af",
   "metadata": {},
   "source": [
    "## Remove Frames till 2000 frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "f4ed2522-f499-4330-95d0-eef79e97eccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "while len(os.listdir(os.path.join(\"english_data\", \"Yes\"))) > 2000:\n",
    "    random_image = random.sample(os.listdir(os.path.join(\"english_data\", \"Yes\")), k=1)\n",
    "    # print(random_image)\n",
    "    random_image_path = os.path.join(\"english_data\", \"Yes\", random_image[0])\n",
    "    os.remove(random_image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0bd64b-2117-4350-860c-969be05628e9",
   "metadata": {},
   "source": [
    "## Check that 2 hands words are detected correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f478d18-86a1-4f72-9658-c9983ee4e976",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in os.listdir(os.path.join(\"arabic_data\", \"pray\")):\n",
    "    image = cv2.imread(os.path.join(\"arabic_data\", \"pray\", i))\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(image_rgb)\n",
    "    # print(i)\n",
    "\n",
    "    if result.multi_hand_landmarks:\n",
    "        extracted_frame_features = []\n",
    "        for hand_landmark in result.multi_hand_landmarks:\n",
    "            for landmark in hand_landmark.landmark:\n",
    "                extracted_frame_features.append(landmark.x)\n",
    "                extracted_frame_features.append(landmark.y)\n",
    "\n",
    "        if len(extracted_frame_features) != 84:\n",
    "            print(i)\n",
    "            print(len(extracted_frame_features))\n",
    "            os.remove(os.path.join(\"arabic_data\", \"pray\", i))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af9e143-b771-4c7d-9e39-0bf1fa96d746",
   "metadata": {},
   "source": [
    "## Check the balancing of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8816daa-52f0-4d74-8d8a-47a9aac13aa1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for class_name in os.listdir(os.path.join('arabic_data')):\n",
    "    if len(os.listdir(os.path.join('arabic_data', class_name))) != 0:\n",
    "        counter = 0\n",
    "        for numpy_array in os.listdir(os.path.join(\"arabic_data\", class_name)):\n",
    "            if numpy_array.split(\".\")[-1] == \"npy\":\n",
    "                counter += 1\n",
    "        \n",
    "        print(class_name)\n",
    "        print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5be931-55e2-4c95-a556-792c0c5fd508",
   "metadata": {},
   "source": [
    "# 3- Extract Features from the images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba03e6d-7c55-4135-a430-8254d3dbfd06",
   "metadata": {},
   "source": [
    "## Process : extract features and save it in .npy file for each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6427883-65da-486e-9211-bfdb4598a5bb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for class_name in os.listdir(os.path.join('arabic_data')):\n",
    "    if len(os.listdir(os.path.join('arabic_data', class_name))) != 0:\n",
    "        print(f\"------------------------------------- {class_name} ---------------------------------\")\n",
    "        for image_name in os.listdir(os.path.join('arabic_data', class_name)):\n",
    "            if image_name.split(\".\")[-1] == \"jpg\":\n",
    "                print(image_name)\n",
    "                # define the path of the image\n",
    "                image_path = os.path.join('arabic_data', class_name, image_name)\n",
    "\n",
    "                # bgr image\n",
    "                image = cv2.imread(image_path)\n",
    "\n",
    "                # rgb image \n",
    "                image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                # process the rgb image to get the hand detection\n",
    "                result = hands.process(image_rgb)\n",
    "\n",
    "                # check if there is any detection of hands or not \n",
    "                if result.multi_hand_landmarks:\n",
    "\n",
    "                    # define list to put in it x,y values for each landmarks \n",
    "                    current_image_landmarks = []\n",
    "\n",
    "                    # get x and y value for each landmark\n",
    "                    for hand_landmark in result.multi_hand_landmarks:\n",
    "                        for landmark in hand_landmark.landmark:\n",
    "                            current_image_landmarks.append(landmark.x)\n",
    "                            current_image_landmarks.append(landmark.y)\n",
    "\n",
    "                    # check that the number of landmarks are equal for each image\n",
    "                    if len(current_image_landmarks) < 84:\n",
    "                        current_image_landmarks = current_image_landmarks + [0]*(84-len(current_image_landmarks))\n",
    "\n",
    "                    # save the extracted image features in numpy array \n",
    "                    save_path = os.path.join(\"arabic_data\", class_name, f\"{image_name}_features\")\n",
    "                    np.save(save_path, current_image_landmarks)\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4c4b3a-d671-448d-b6ae-921cc49e600b",
   "metadata": {},
   "source": [
    "## Create label map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "6d47bf27-768f-4e15-a1cd-ff393894f0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "['aeen', 'alef', 'allah', 'bad', 'bank', 'bathroom', 'beh', 'cairo', 'chair', 'child', 'college', 'daaad', 'daal', 'deaf', 'dish', 'divorce', 'egypt', 'eight', 'engagement', 'engineer', 'father', 'feh', 'five', 'four', 'geem', 'gheen', 'girl_or_daughter', 'green', 'haa', 'happy', 'hehh', 'help', 'home_or_house', 'hospital', 'hungry', 'ismaalia', 'i_love_you', 'i_or_me', 'kaaf', 'khaa', 'laam', 'luxor', 'meem', 'monday', 'nine', 'noon', 'one', 'orange', 'pray', 'qaaf', 'raa', 'saaad', 'saturday', 'school', 'seen', 'seven', 'sharkia', 'sheen', 'sign', 'six', 'sleep_n_v', 'son_or_boy', 'tah', 'teacher', 'teen_fruit', 'teh', 'ten', 'theh', 'the_peace', 'three', 'thursday', 'tuesday', 'tv', 'two', 'university', 'waow', 'water', 'wednesday', 'what', 'where', 'white', 'work_v', 'yeeh', 'yellow', 'yes', 'you', 'your_name', 'zaal', 'zah', 'zeen']\n"
     ]
    }
   ],
   "source": [
    "classes = []\n",
    "for class_name in os.listdir(os.path.join('arabic_data')):\n",
    "    if len(os.listdir(os.path.join('arabic_data', class_name))) != 0:\n",
    "        classes.append(class_name)\n",
    "\n",
    "print(len(classes))\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "13683c59-c27d-48f5-8259-f04d247499e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'aeen': 0, 'alef': 1, 'allah': 2, 'bad': 3, 'bank': 4, 'bathroom': 5, 'beh': 6, 'cairo': 7, 'chair': 8, 'child': 9, 'college': 10, 'daaad': 11, 'daal': 12, 'deaf': 13, 'dish': 14, 'divorce': 15, 'egypt': 16, 'eight': 17, 'engagement': 18, 'engineer': 19, 'father': 20, 'feh': 21, 'five': 22, 'four': 23, 'geem': 24, 'gheen': 25, 'girl_or_daughter': 26, 'green': 27, 'haa': 28, 'happy': 29, 'hehh': 30, 'help': 31, 'home_or_house': 32, 'hospital': 33, 'hungry': 34, 'ismaalia': 35, 'i_love_you': 36, 'i_or_me': 37, 'kaaf': 38, 'khaa': 39, 'laam': 40, 'luxor': 41, 'meem': 42, 'monday': 43, 'nine': 44, 'noon': 45, 'one': 46, 'orange': 47, 'pray': 48, 'qaaf': 49, 'raa': 50, 'saaad': 51, 'saturday': 52, 'school': 53, 'seen': 54, 'seven': 55, 'sharkia': 56, 'sheen': 57, 'sign': 58, 'six': 59, 'sleep_n_v': 60, 'son_or_boy': 61, 'tah': 62, 'teacher': 63, 'teen_fruit': 64, 'teh': 65, 'ten': 66, 'theh': 67, 'the_peace': 68, 'three': 69, 'thursday': 70, 'tuesday': 71, 'tv': 72, 'two': 73, 'university': 74, 'waow': 75, 'water': 76, 'wednesday': 77, 'what': 78, 'where': 79, 'white': 80, 'work_v': 81, 'yeeh': 82, 'yellow': 83, 'yes': 84, 'you': 85, 'your_name': 86, 'zaal': 87, 'zah': 88, 'zeen': 89}\n"
     ]
    }
   ],
   "source": [
    "class_to_index = {class_name:index for index, class_name in enumerate(classes)}\n",
    "print(class_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2983dd-4d1b-47fe-887c-bfe7e0dd32f2",
   "metadata": {},
   "source": [
    "## Collect all the features in one array called \"all_features\" with thier labels in \"all_labels\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5eadf4b-9726-41e1-a234-b61b7521eb70",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_features = []\n",
    "all_labels = []\n",
    "\n",
    "for class_name in os.listdir(os.path.join('arabic_data')):\n",
    "    print(f\"-------------------------------- {class_name} -----------------------------\")\n",
    "    for numpy_file in os.listdir(os.path.join('arabic_data', class_name)):\n",
    "        if numpy_file.split(\".\")[-1] == \"npy\":\n",
    "            # print(numpy_file)\n",
    "            # load the numpy file \n",
    "            numpy_file_path = os.path.join('arabic_data', class_name, numpy_file)\n",
    "            numpy_array = np.load(numpy_file_path)\n",
    "            all_features.append(numpy_array)\n",
    "            all_labels.append(class_to_index[class_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfcb205-e237-4317-9206-c66a78019786",
   "metadata": {},
   "source": [
    "## Save all_features and all_labels lists to external file called \"english_data.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "fdc4ec9e-d170-4dbe-a9d4-0bb62548ae26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all_landmarks and labels in file called data.pickle using pickle module \n",
    "data_file = open('arabic_data.pickle','wb')\n",
    "pickle.dump({'all_features':all_features, 'all_labels':all_labels}, data_file)\n",
    "data_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dc8e41-05c9-4001-8a64-b881fe4f50e5",
   "metadata": {},
   "source": [
    "## Load all_features and all_labels to use them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "174cc12b-e773-4b26-9ae2-3caaced03b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the values from the file \n",
    "data_file = open('arabic_data.pickle', 'rb')\n",
    "data = pickle.load(data_file)\n",
    "\n",
    "all_features = data['all_features']\n",
    "all_labels = data['all_labels']\n",
    "\n",
    "data_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967d044e-aaac-40e5-af3f-9149ae0d3a17",
   "metadata": {},
   "source": [
    "## Convert all_features and all_labels to arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd042baa-8f8c-4f5c-910b-e3eebd1f54d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(180000, 84)\n",
      "(180000,)\n"
     ]
    }
   ],
   "source": [
    "# convert the data to arrays \n",
    "all_features_array = np.array(all_features)\n",
    "all_labels_array = np.array(all_labels)\n",
    "\n",
    "print(type(all_features_array))\n",
    "print(type(all_labels_array))\n",
    "\n",
    "print(all_features_array.shape)\n",
    "print(all_labels_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1947b79f-da7b-4527-97d0-30107890ee57",
   "metadata": {},
   "source": [
    "# 4- Dataset Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06dda460-0706-4c6a-a665-836d71aafb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afe8c151-33fd-440a-9fab-bac0d9721b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(all_features_array, all_labels_array, test_size=0.2, shuffle=True, stratify=all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b747331d-262d-4dc1-81f7-c97076a1d884",
   "metadata": {},
   "source": [
    "# 5- Model Building "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0eff80b-1fb9-4b43-ba9d-5c8016b210d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9b1a7cb-3e3f-4224-a176-51890b5f2db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_random_forest = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ea5e87e-38f6-43d4-9ba7-cf35790afb0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;RandomForestClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">?<span>Documentation for RandomForestClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_random_forest.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd293369-f0c4-45a9-8e20-d7741dc2aea1",
   "metadata": {},
   "source": [
    "## Save the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3f462d5-7b87-476e-a55a-5d7051dee904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "model_file = open('arabic_model.pickle','wb')\n",
    "pickle.dump({'model':model_random_forest}, model_file)\n",
    "model_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2285ce4b-6c7a-40c6-abe9-d98e12bb445a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model_file = open('arabic_model.pickle','rb')\n",
    "model_dict = pickle.load(model_file)\n",
    "model = model_dict['model']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b723053-09bb-48bc-93a5-9c265e81c32f",
   "metadata": {},
   "source": [
    "# 6-Model Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb11ee67-90cf-4705-a674-522c59713de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e20f91-d34c-485c-90f6-78dde7f30cf6",
   "metadata": {},
   "source": [
    "## Calculate y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6588314-d735-487a-a4e1-313c5547a3ff",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the y_predict \n",
    "y_predict = model.predict(x_test)\n",
    "print(y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58962e6e-acb2-41f8-929b-463dc14743dd",
   "metadata": {},
   "source": [
    "## Calculate confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d516d33e-1ccb-48bd-b429-f53b78105d32",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_predict)\n",
    "for i in cm:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a9186f-ee55-4be9-beb9-6fca212b7b5f",
   "metadata": {},
   "source": [
    "## Calculate Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3e6fb47-001d-4db1-ae62-5b4f8a4a353b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9988888888888889\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_predict)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ae2eb6-ec0a-490f-8294-529ba5511742",
   "metadata": {},
   "source": [
    "## Calculate Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24837c1c-6706-419e-b7bf-934388ffd962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9988888888888889\n"
     ]
    }
   ],
   "source": [
    "precision = precision_score(y_test, y_predict, average='micro')\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818461cd-7d89-499b-8aba-4b743edcdda6",
   "metadata": {},
   "source": [
    "## Calculate F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2aa4fac7-eda2-4b4f-98ea-88c93916a75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9988888888888889\n"
     ]
    }
   ],
   "source": [
    "f1 = f1_score(y_test, y_predict, average='micro')\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89b45b9-1e38-49d7-9a27-049f1b631fa3",
   "metadata": {},
   "source": [
    "## Calculate Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3f60d70-2c7f-4877-8da5-41d8a03e82a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9988888888888889\n"
     ]
    }
   ],
   "source": [
    "recall = recall_score(y_test, y_predict, average='micro')\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801d7a93-64cf-4f71-9931-04891385eecd",
   "metadata": {},
   "source": [
    "## Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c041a98b-4686-40e5-ad47-1e31055a8cde",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       400\n",
      "           1       1.00      1.00      1.00       400\n",
      "           2       1.00      1.00      1.00       400\n",
      "           3       1.00      0.99      0.99       400\n",
      "           4       1.00      1.00      1.00       400\n",
      "           5       1.00      0.98      0.99       400\n",
      "           6       1.00      1.00      1.00       400\n",
      "           7       1.00      1.00      1.00       400\n",
      "           8       1.00      1.00      1.00       400\n",
      "           9       1.00      1.00      1.00       400\n",
      "          10       1.00      1.00      1.00       400\n",
      "          11       1.00      1.00      1.00       400\n",
      "          12       1.00      1.00      1.00       400\n",
      "          13       1.00      1.00      1.00       400\n",
      "          14       1.00      1.00      1.00       400\n",
      "          15       1.00      1.00      1.00       400\n",
      "          16       1.00      0.99      1.00       400\n",
      "          17       1.00      1.00      1.00       400\n",
      "          18       1.00      1.00      1.00       400\n",
      "          19       1.00      1.00      1.00       400\n",
      "          20       0.99      1.00      0.99       400\n",
      "          21       0.99      1.00      1.00       400\n",
      "          22       1.00      1.00      1.00       400\n",
      "          23       1.00      1.00      1.00       400\n",
      "          24       1.00      1.00      1.00       400\n",
      "          25       1.00      1.00      1.00       400\n",
      "          26       1.00      0.99      1.00       400\n",
      "          27       1.00      1.00      1.00       400\n",
      "          28       1.00      1.00      1.00       400\n",
      "          29       1.00      1.00      1.00       400\n",
      "          30       1.00      1.00      1.00       400\n",
      "          31       1.00      1.00      1.00       400\n",
      "          32       1.00      1.00      1.00       400\n",
      "          33       1.00      1.00      1.00       400\n",
      "          34       1.00      1.00      1.00       400\n",
      "          35       1.00      1.00      1.00       400\n",
      "          36       1.00      1.00      1.00       400\n",
      "          37       1.00      1.00      1.00       400\n",
      "          38       1.00      1.00      1.00       400\n",
      "          39       1.00      1.00      1.00       400\n",
      "          40       1.00      1.00      1.00       400\n",
      "          41       1.00      1.00      1.00       400\n",
      "          42       1.00      1.00      1.00       400\n",
      "          43       1.00      1.00      1.00       400\n",
      "          44       1.00      1.00      1.00       400\n",
      "          45       1.00      1.00      1.00       400\n",
      "          46       1.00      1.00      1.00       400\n",
      "          47       1.00      1.00      1.00       400\n",
      "          48       1.00      1.00      1.00       400\n",
      "          49       1.00      1.00      1.00       400\n",
      "          50       1.00      1.00      1.00       400\n",
      "          51       1.00      1.00      1.00       400\n",
      "          52       1.00      1.00      1.00       400\n",
      "          53       1.00      1.00      1.00       400\n",
      "          54       1.00      1.00      1.00       400\n",
      "          55       1.00      1.00      1.00       400\n",
      "          56       1.00      1.00      1.00       400\n",
      "          57       1.00      1.00      1.00       400\n",
      "          58       1.00      0.99      0.99       400\n",
      "          59       1.00      1.00      1.00       400\n",
      "          60       0.99      0.99      0.99       400\n",
      "          61       1.00      1.00      1.00       400\n",
      "          62       1.00      1.00      1.00       400\n",
      "          63       1.00      1.00      1.00       400\n",
      "          64       1.00      1.00      1.00       400\n",
      "          65       1.00      1.00      1.00       400\n",
      "          66       1.00      1.00      1.00       400\n",
      "          67       1.00      1.00      1.00       400\n",
      "          68       1.00      1.00      1.00       400\n",
      "          69       1.00      1.00      1.00       400\n",
      "          70       1.00      0.99      1.00       400\n",
      "          71       1.00      1.00      1.00       400\n",
      "          72       1.00      1.00      1.00       400\n",
      "          73       1.00      1.00      1.00       400\n",
      "          74       1.00      0.99      1.00       400\n",
      "          75       1.00      1.00      1.00       400\n",
      "          76       1.00      0.99      1.00       400\n",
      "          77       1.00      1.00      1.00       400\n",
      "          78       1.00      1.00      1.00       400\n",
      "          79       0.99      1.00      0.99       400\n",
      "          80       1.00      1.00      1.00       400\n",
      "          81       1.00      1.00      1.00       400\n",
      "          82       1.00      1.00      1.00       400\n",
      "          83       1.00      1.00      1.00       400\n",
      "          84       1.00      1.00      1.00       400\n",
      "          85       1.00      1.00      1.00       400\n",
      "          86       1.00      1.00      1.00       400\n",
      "          87       1.00      1.00      1.00       400\n",
      "          88       1.00      1.00      1.00       400\n",
      "          89       1.00      1.00      1.00       400\n",
      "\n",
      "    accuracy                           1.00     36000\n",
      "   macro avg       1.00      1.00      1.00     36000\n",
      "weighted avg       1.00      1.00      1.00     36000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, y_predict)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f15bbf8-7ee1-41fc-8a5a-ca9ea2d68531",
   "metadata": {},
   "source": [
    "# 7- Test model on videos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68f25c3-11a4-4221-a602-4022c2b5f7e6",
   "metadata": {},
   "source": [
    "## Load google model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "10c2ea87-7ecd-4637-8042-a29ade23626a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create object just focus on the hands \n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# get the model that detect hand_landmarks \n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b260534-1804-45af-919b-cbbd542f2626",
   "metadata": {},
   "source": [
    "## Load my model model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "65c6e89d-2578-48c4-af23-2df5f4a6dc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model_file = open('arabic_model.pickle','rb')\n",
    "model_dict = pickle.load(model_file)\n",
    "model = model_dict['model']\n",
    "model_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2cdc96-6171-40ab-819d-10dafd385231",
   "metadata": {},
   "source": [
    "## Create label map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a579de60-ff24-4b9e-8208-3abc6a67139b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aeen', 'alef', 'allah', 'bad', 'bank', 'bathroom', 'beh', 'cairo', 'chair', 'child', 'college', 'daaad', 'daal', 'deaf', 'dish', 'divorce', 'egypt', 'eight', 'engagement', 'engineer', 'father', 'feh', 'five', 'four', 'geem', 'gheen', 'girl_or_daughter', 'green', 'haa', 'happy', 'hehh', 'help', 'home_or_house', 'hospital', 'hungry', 'ismaalia', 'i_love_you', 'i_or_me', 'kaaf', 'khaa', 'laam', 'luxor', 'meem', 'monday', 'nine', 'noon', 'one', 'orange', 'pray', 'qaaf', 'raa', 'saaad', 'saturday', 'school', 'seen', 'seven', 'sharkia', 'sheen', 'sign', 'six', 'sleep_n_v', 'son_or_boy', 'tah', 'teacher', 'teen_fruit', 'teh', 'ten', 'theh', 'the_peace', 'three', 'thursday', 'tuesday', 'tv', 'two', 'university', 'waow', 'water', 'wednesday', 'what', 'where', 'white', 'work_v', 'yeeh', 'yellow', 'yes', 'you', 'your_name', 'zaal', 'zah', 'zeen']\n"
     ]
    }
   ],
   "source": [
    "classes = []\n",
    "for class_name in os.listdir(os.path.join('arabic_data')):\n",
    "    if len(os.listdir(os.path.join('arabic_data', class_name))) != 0:\n",
    "        classes.append(class_name)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "736a6916-8a8b-4f76-b19e-9a911b5928ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'aeen', 1: 'alef', 2: 'allah', 3: 'bad', 4: 'bank', 5: 'bathroom', 6: 'beh', 7: 'cairo', 8: 'chair', 9: 'child', 10: 'college', 11: 'daaad', 12: 'daal', 13: 'deaf', 14: 'dish', 15: 'divorce', 16: 'egypt', 17: 'eight', 18: 'engagement', 19: 'engineer', 20: 'father', 21: 'feh', 22: 'five', 23: 'four', 24: 'geem', 25: 'gheen', 26: 'girl_or_daughter', 27: 'green', 28: 'haa', 29: 'happy', 30: 'hehh', 31: 'help', 32: 'home_or_house', 33: 'hospital', 34: 'hungry', 35: 'ismaalia', 36: 'i_love_you', 37: 'i_or_me', 38: 'kaaf', 39: 'khaa', 40: 'laam', 41: 'luxor', 42: 'meem', 43: 'monday', 44: 'nine', 45: 'noon', 46: 'one', 47: 'orange', 48: 'pray', 49: 'qaaf', 50: 'raa', 51: 'saaad', 52: 'saturday', 53: 'school', 54: 'seen', 55: 'seven', 56: 'sharkia', 57: 'sheen', 58: 'sign', 59: 'six', 60: 'sleep_n_v', 61: 'son_or_boy', 62: 'tah', 63: 'teacher', 64: 'teen_fruit', 65: 'teh', 66: 'ten', 67: 'theh', 68: 'the_peace', 69: 'three', 70: 'thursday', 71: 'tuesday', 72: 'tv', 73: 'two', 74: 'university', 75: 'waow', 76: 'water', 77: 'wednesday', 78: 'what', 79: 'where', 80: 'white', 81: 'work_v', 82: 'yeeh', 83: 'yellow', 84: 'yes', 85: 'you', 86: 'your_name', 87: 'zaal', 88: 'zah', 89: 'zeen'}\n"
     ]
    }
   ],
   "source": [
    "index_to_class = {index:class_name for index, class_name in enumerate(classes)}\n",
    "print(index_to_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be76ebb-e348-4d67-8f54-2d6570df65e4",
   "metadata": {},
   "source": [
    "## Convert english text to arabic text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "66843cb4-4aed-4c8e-a0dc-8aa231f87855",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_to_arabic = {'aeen':display_arabic_text('ع'), 'alef':display_arabic_text('أ'), 'allah':display_arabic_text('الله'),\n",
    "                    'bad':display_arabic_text('سىء'), 'bank':display_arabic_text('بنك'), 'bathroom':display_arabic_text('حمام'),\n",
    "                    'beh':display_arabic_text('ب'), 'cairo':display_arabic_text('القاهرة'), 'chair':display_arabic_text('كرسي'),\n",
    "                    'child':display_arabic_text('طفل'), 'college':display_arabic_text('كلية'), 'daaad':display_arabic_text('ض'),\n",
    "                    'daal':display_arabic_text('د'), 'deaf':display_arabic_text('أصم'), 'dish':display_arabic_text('طبق'),\n",
    "                    'divorce':display_arabic_text('طلاق/مطلق/مطلقة'), 'egypt':display_arabic_text('مصر'), 'eight':display_arabic_text('ثمانية'),\n",
    "                    'engagement':display_arabic_text('خطوبة'), 'engineer':display_arabic_text('مهندس'), 'father':display_arabic_text('أب'),\n",
    "                    'feh':display_arabic_text('ف'), 'five':display_arabic_text('خمسة'), 'four':display_arabic_text('أربعة'),\n",
    "                    'geem':display_arabic_text('ج'), 'gheen':display_arabic_text('غ'), 'girl_or_daughter':display_arabic_text('بنت/أبنة'),\n",
    "                    'green':display_arabic_text('أخضر'), 'haa':display_arabic_text('ح'), 'happy':display_arabic_text('سعيد'),\n",
    "                    'hehh':display_arabic_text('ه'), 'help':display_arabic_text('مساعدة'), 'home_or_house':display_arabic_text('منزل/بيت'),\n",
    "                    'hospital':display_arabic_text('مستشفى'), 'hungry':display_arabic_text('جائع'), 'ismaalia':display_arabic_text('الاسماعيلية'),\n",
    "                    'i_love_you':display_arabic_text('أحبك'), 'i_or_me':display_arabic_text('انا'), 'kaaf':display_arabic_text('ك'),\n",
    "                    'khaa':display_arabic_text('خ'), 'laam':display_arabic_text('ل'), 'luxor':display_arabic_text('الأقصر'),\n",
    "                    'meem':display_arabic_text('م'), 'monday':display_arabic_text('الأثنين'), 'nine':display_arabic_text('تسعة'),\n",
    "                    'noon':display_arabic_text('ن'), 'one':display_arabic_text('واحد'), 'orange':display_arabic_text('برتقال'),\n",
    "                    'pray':display_arabic_text('الصلاة'), 'qaaf':display_arabic_text('ق'), 'raa':display_arabic_text('ر'),\n",
    "                    'saaad':display_arabic_text('ص'), 'saturday':display_arabic_text('السبت'), 'school':display_arabic_text('مدرسة'),\n",
    "                    'seen':display_arabic_text('س'), 'seven':display_arabic_text('سبعة'), 'sharkia':display_arabic_text('الشرقية'),\n",
    "                    'sheen':display_arabic_text('ش'), 'sign':display_arabic_text('اشارة'), 'six':display_arabic_text('ستة'),\n",
    "                    'sleep_n_v':display_arabic_text('ينام/نوم'), 'son_or_boy':display_arabic_text('أبن/ولد'), 'tah':display_arabic_text('ط'),\n",
    "                    'teacher':display_arabic_text('أستاذ/معلم'), 'teen_fruit':display_arabic_text('تين'), 'teh':display_arabic_text('ت'),\n",
    "                    'ten':display_arabic_text('عشرة'), 'theh':display_arabic_text('ث'), 'the_peace':display_arabic_text('السلام'),\n",
    "                    'three':display_arabic_text('ثلاثة'), 'thursday':display_arabic_text('الخميس'), 'tuesday':display_arabic_text('الثلاثاء'),\n",
    "                    'tv':display_arabic_text('التيليفزيون'), 'two':display_arabic_text('أثنين'), 'university':display_arabic_text('الجامعة'),\n",
    "                    'waow':display_arabic_text('و'), 'water':display_arabic_text('ماء'), 'wednesday':display_arabic_text('الأربعاء'),\n",
    "                    'what':display_arabic_text('أيه/ماذا'), 'where':display_arabic_text('فين/أين'), 'white':display_arabic_text('أبيض'),\n",
    "                    'work_v':display_arabic_text('يعمل/يشتغل'), 'yeeh':display_arabic_text('ي'), 'yellow':display_arabic_text('أصفر'),\n",
    "                    'yes':display_arabic_text('نعم'), 'you':display_arabic_text('أنت'), 'your_name':display_arabic_text('أسمك'),\n",
    "                    'zaal':display_arabic_text('ذ'), 'zah':display_arabic_text('ظ'), 'zeen':display_arabic_text('ز')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "baeb0f5c-6d21-4368-a86e-a7e6008031aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_english_to_arabic = {'aeen':'ع', 'alef':'أ', 'allah':'الله',\n",
    "                    'bad':'سىء', 'bank':'بنك', 'bathroom':'حمام',\n",
    "                    'beh':'ب', 'cairo':'القاهرة', 'chair':'كرسي',\n",
    "                    'child':'طفل', 'college':'كلية', 'daaad':'ض',\n",
    "                    'daal':'د', 'deaf':'أصم', 'dish':'طبق',\n",
    "                    'divorce':'طلاق/مطلق/مطلقة', 'egypt':'مصر', 'eight':'ثمانية',\n",
    "                    'engagement':'خطوبة', 'engineer':'مهندس', 'father':'أب',\n",
    "                    'feh':'ف', 'five':'خمسة', 'four':'أربعة',\n",
    "                    'geem':'ج', 'gheen':'غ', 'girl_or_daughter':'بنت/أبنة',\n",
    "                    'green':'أخضر', 'haa':'ح', 'happy':'سعيد',\n",
    "                    'hehh':'ه', 'help':'مساعدة', 'home_or_house':'منزل/بيت',\n",
    "                    'hospital':'مستشفى', 'hungry':'جائع', 'ismaalia':'الاسماعيلية',\n",
    "                    'i_love_you':'أحبك', 'i_or_me':'انا', 'kaaf':'ك',\n",
    "                    'khaa':'خ', 'laam':'ل', 'luxor':'الأقصر',\n",
    "                    'meem':'م', 'monday':'الأثنين', 'nine':'تسعة',\n",
    "                    'noon':'ن', 'one':'واحد', 'orange':'برتقال',\n",
    "                    'pray':'الصلاة', 'qaaf':'ق', 'raa':'ر',\n",
    "                    'saaad':'ص', 'saturday':'السبت', 'school':'مدرسة',\n",
    "                    'seen':'س', 'seven':'سبعة', 'sharkia':'الشرقية',\n",
    "                    'sheen':'ش', 'sign':'اشارة', 'six':'ستة',\n",
    "                    'sleep_n_v':'ينام/نوم', 'son_or_boy':'أبن/ولد', 'tah':'ط',\n",
    "                    'teacher':'أستاذ/معلم', 'teen_fruit':'تين', 'teh':'ت',\n",
    "                    'ten':'عشرة', 'theh':'ث', 'the_peace':'السلام',\n",
    "                    'three':'ثلاثة', 'thursday':'الخميس', 'tuesday':'الثلاثاء',\n",
    "                    'tv':'التيليفزيون', 'two':'أثنين', 'university':'الجامعة',\n",
    "                    'waow':'و', 'water':'ماء', 'wednesday':'الأربعاء',\n",
    "                    'what':'أيه/ماذا', 'where':'فين/أين', 'white':'أبيض',\n",
    "                    'work_v':'يعمل/يشتغل', 'yeeh':'ي', 'yellow':'أصفر',\n",
    "                    'yes':'نعم', 'you':'أنت', 'your_name':'أسمك',\n",
    "                    'zaal':'ذ', 'zah':'ظ', 'zeen':'ز'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa3610f-0e8d-4c4b-8d50-7e159b6458ef",
   "metadata": {},
   "source": [
    "## Function: take image and output label and its probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f0578af2-71f5-4173-b1d4-a183d6a2e1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define method to predict image with the random forest model\n",
    "# image -> label, probablity of it \n",
    "\n",
    "def model_image_predict(image):\n",
    "    \n",
    "    # convert image to rgb \n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # get landmarks\n",
    "    result = hands.process(image_rgb)\n",
    "    \n",
    "    # define list to put all the landmarks in it \n",
    "    all_features = []\n",
    "    \n",
    "    # define list to put in it x,y values for each landmarks \n",
    "    current_image_landmarks = []\n",
    "\n",
    "    # get x and y value for each landmark\n",
    "\n",
    "    # check if there is any detection of hands or not \n",
    "    if result.multi_hand_landmarks:\n",
    "\n",
    "        for hand_landmark in result.multi_hand_landmarks:\n",
    "            for landmark in hand_landmark.landmark:\n",
    "                current_image_landmarks.append(landmark.x)\n",
    "                current_image_landmarks.append(landmark.y)\n",
    "\n",
    "        # check that the number of landmarks are equal for each image\n",
    "        if len(current_image_landmarks) < 84:\n",
    "            current_image_landmarks = current_image_landmarks + [0]*(84-len(current_image_landmarks))\n",
    "\n",
    "        # append the value of current_image_data in the all_data list\n",
    "        all_features.append(current_image_landmarks)\n",
    "        \n",
    "        # convert the all_landmarks from list to 2d array\n",
    "        all_features_array = np.array(all_features)\n",
    "        \n",
    "        \n",
    "        prediction = model.predict(all_features_array)\n",
    "        prediction_with_probability = model.predict_proba(all_features_array)\n",
    "\n",
    "        return {'class':prediction[0], 'probability':prediction_with_probability[0][prediction[0]]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541f7217-6d5d-43c2-8b6e-b53097c603af",
   "metadata": {},
   "source": [
    "## Predict on image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "23c23f10-f539-4eb9-97e0-8743ec41589f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class': 0, 'probability': 1.0}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "image_path = os.path.join(\"arabic_data\", \"aeen\", \"new_left_3een_frame_1.jpg\")\n",
    "image = cv2.imread(image_path)\n",
    "prediction = model_image_predict(image)\n",
    "print(prediction)\n",
    "print(type(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33ac797-b40a-4996-8681-850ca923df39",
   "metadata": {},
   "source": [
    "## Function: Convert the video to text\n",
    "Be attention for 2 important things :\n",
    "- Frame Per Second [FPS]\n",
    "- Video Status: \n",
    "    - 1 sign language \n",
    "    - Multiple sign language "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dbcc7b14-987b-4efe-9d60-3cede8a13ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_to_text_prediction(video_path):\n",
    "\n",
    "    # read the video\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # calculate the frame per second of the video -> take 1 frame per 1/2 second\n",
    "    fps = round(round(video.get(cv2.CAP_PROP_FPS))/2) # 30 frmas per sec [in each second we will get 2 frames for detection process]\n",
    "\n",
    "    # define frame_counter variable --> when it reach 15 we will take this frame [so we take a frame after 1/2 second]\n",
    "    frame_counter = 0\n",
    "\n",
    "    # get all predictions from the video in all_prediction dictionary \n",
    "    all_predictions = dict()\n",
    "    prediction_id = 0\n",
    "\n",
    "    status = True  \n",
    "    while status:\n",
    "        # read frames from the video\n",
    "        status, frame = video.read()\n",
    "\n",
    "        if status == True:\n",
    "            # count the current frame\n",
    "            frame_counter += 1\n",
    "            # here is the frame that will be used for prediction\n",
    "            if frame_counter % fps == 0:\n",
    "                prediction = model_image_predict(frame)\n",
    "                prediction_id += 1 \n",
    "                all_predictions[prediction_id] = prediction\n",
    "    \n",
    "    \n",
    "    threshold = 0.25\n",
    "    all_classes = str()\n",
    "    for i in all_predictions:\n",
    "        # check if there is a prediction or not\n",
    "        if all_predictions[i]:\n",
    "            if all_predictions[i]['probability'] > threshold:\n",
    "                arabic_text = new_english_to_arabic[index_to_class[all_predictions[i]['class']]]\n",
    "                all_classes += arabic_text + ' '\n",
    "    # make unique classes [Cancel repetition]\n",
    "    previous_class = None\n",
    "    final_classes = []\n",
    "\n",
    "    for i in all_classes.rstrip().split():\n",
    "\n",
    "        if i != previous_class:\n",
    "            final_classes.append(i)\n",
    "            previous_class = i\n",
    "\n",
    "    return \" \".join(final_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebcf916-3781-4933-bd1f-102b965c8724",
   "metadata": {},
   "source": [
    "## Predict on videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "519370f2-d4e7-4ebd-b582-e49e9e6b8637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "واحد أثنين ثلاثة\n"
     ]
    }
   ],
   "source": [
    "print(video_to_text_prediction(os.path.join(\"arabic_test_videos\", \"واحد_أثنين_ثلاثة.mp4\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c269bb-cb00-4100-b37d-f65e6d82c31e",
   "metadata": {},
   "source": [
    "# 8- Video to text model Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9554b9d-2e35-4d61-8111-27abf31db912",
   "metadata": {},
   "source": [
    "## Endpoint video to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "526561a8-d103-4fe3-b542-704a533b9b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from flask import Flask, request, jsonify\n",
    "from celery import Celery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c2be6a1-03a2-42c5-a025-29d46fbc5694",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/video_to_text_arabic\", methods=['POST'])\n",
    "def video_to_text():\n",
    "\n",
    "    # upload file\n",
    "    # this is the video itself\n",
    "    video = request.files['video']\n",
    "    file_name = video.filename.split(\"/\")[-1]\n",
    "    \n",
    "    # make a fixed video name to make overrite and make sure that the uploaded file just have only 1 video file\n",
    "    file_name = \"new_video.mp4\"\n",
    "    \n",
    "    print(\"this is the path of the video\", file_name)\n",
    "\n",
    "    # save the video -> to access it and make the detection process\n",
    "    # after saving the video we will get the 1st parameter for the detection method -> [video_path]\n",
    "    \n",
    "   # save the new video\n",
    "    video.save(os.path.join('uploaded', file_name))\n",
    "        \n",
    "        \n",
    "\n",
    "    # apply model on it\n",
    "    output = video_to_text_prediction(os.path.join('uploaded', file_name))\n",
    "    \n",
    "    return jsonify([{\"text\":output}])\n",
    "\n",
    "\n",
    "app.run(port=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a11e85-a78a-444d-89b4-09c7efe19d0b",
   "metadata": {},
   "source": [
    "# 9- Test in real time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e1665dfb-b9b9-44d4-ae4e-c3749a7e512c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# load model \n",
    "model_file = open('arabic_model.pickle','rb')\n",
    "model_dict = pickle.load(model_file)\n",
    "model = model_dict['model']\n",
    "model_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b9fa246a-d60c-4b28-b937-d58d1c49262e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aeen', 'alef', 'allah', 'bad', 'bank', 'bathroom', 'beh', 'cairo', 'chair', 'child', 'college', 'daaad', 'daal', 'deaf', 'dish', 'divorce', 'egypt', 'eight', 'engagement', 'engineer', 'father', 'feh', 'five', 'four', 'geem', 'gheen', 'girl_or_daughter', 'green', 'haa', 'happy', 'hehh', 'help', 'home_or_house', 'hospital', 'hungry', 'ismaalia', 'i_love_you', 'i_or_me', 'kaaf', 'khaa', 'laam', 'luxor', 'meem', 'monday', 'nine', 'noon', 'one', 'orange', 'pray', 'qaaf', 'raa', 'saaad', 'saturday', 'school', 'seen', 'seven', 'sharkia', 'sheen', 'sign', 'six', 'sleep_n_v', 'son_or_boy', 'tah', 'teacher', 'teen_fruit', 'teh', 'ten', 'theh', 'the_peace', 'three', 'thursday', 'tuesday', 'tv', 'two', 'university', 'waow', 'water', 'wednesday', 'what', 'where', 'white', 'work_v', 'yeeh', 'yellow', 'yes', 'you', 'your_name', 'zaal', 'zah', 'zeen']\n"
     ]
    }
   ],
   "source": [
    "classes = []\n",
    "for class_name in os.listdir(os.path.join('arabic_data')):\n",
    "    if len(os.listdir(os.path.join('arabic_data', class_name))) != 0:\n",
    "        classes.append(class_name)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "82286f45-b8ed-400a-a5d4-209c51445631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'aeen', 1: 'alef', 2: 'allah', 3: 'bad', 4: 'bank', 5: 'bathroom', 6: 'beh', 7: 'cairo', 8: 'chair', 9: 'child', 10: 'college', 11: 'daaad', 12: 'daal', 13: 'deaf', 14: 'dish', 15: 'divorce', 16: 'egypt', 17: 'eight', 18: 'engagement', 19: 'engineer', 20: 'father', 21: 'feh', 22: 'five', 23: 'four', 24: 'geem', 25: 'gheen', 26: 'girl_or_daughter', 27: 'green', 28: 'haa', 29: 'happy', 30: 'hehh', 31: 'help', 32: 'home_or_house', 33: 'hospital', 34: 'hungry', 35: 'ismaalia', 36: 'i_love_you', 37: 'i_or_me', 38: 'kaaf', 39: 'khaa', 40: 'laam', 41: 'luxor', 42: 'meem', 43: 'monday', 44: 'nine', 45: 'noon', 46: 'one', 47: 'orange', 48: 'pray', 49: 'qaaf', 50: 'raa', 51: 'saaad', 52: 'saturday', 53: 'school', 54: 'seen', 55: 'seven', 56: 'sharkia', 57: 'sheen', 58: 'sign', 59: 'six', 60: 'sleep_n_v', 61: 'son_or_boy', 62: 'tah', 63: 'teacher', 64: 'teen_fruit', 65: 'teh', 66: 'ten', 67: 'theh', 68: 'the_peace', 69: 'three', 70: 'thursday', 71: 'tuesday', 72: 'tv', 73: 'two', 74: 'university', 75: 'waow', 76: 'water', 77: 'wednesday', 78: 'what', 79: 'where', 80: 'white', 81: 'work_v', 82: 'yeeh', 83: 'yellow', 84: 'yes', 85: 'you', 86: 'your_name', 87: 'zaal', 88: 'zah', 89: 'zeen'}\n"
     ]
    }
   ],
   "source": [
    "index_to_class = {index:class_name for index, class_name in enumerate(classes)}\n",
    "print(index_to_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18163e80-db36-4bb9-a4e1-5797cc33064f",
   "metadata": {},
   "source": [
    "## Convert english text to arabic text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "861f6ef2-e4de-4065-84e0-58e96e9838db",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_to_arabic = {'aeen':display_arabic_text('ع'), 'alef':display_arabic_text('أ'), 'allah':display_arabic_text('الله'),\n",
    "                    'bad':display_arabic_text('سىء'), 'bank':display_arabic_text('بنك'), 'bathroom':display_arabic_text('حمام'),\n",
    "                    'beh':display_arabic_text('ب'), 'cairo':display_arabic_text('القاهرة'), 'chair':display_arabic_text('كرسي'),\n",
    "                    'child':display_arabic_text('طفل'), 'college':display_arabic_text('كلية'), 'daaad':display_arabic_text('ض'),\n",
    "                    'daal':display_arabic_text('د'), 'deaf':display_arabic_text('أصم'), 'dish':display_arabic_text('طبق'),\n",
    "                    'divorce':display_arabic_text('طلاق/مطلق/مطلقة'), 'egypt':display_arabic_text('مصر'), 'eight':display_arabic_text('ثمانية'),\n",
    "                    'engagement':display_arabic_text('خطوبة'), 'engineer':display_arabic_text('مهندس'), 'father':display_arabic_text('أب'),\n",
    "                    'feh':display_arabic_text('ف'), 'five':display_arabic_text('خمسة'), 'four':display_arabic_text('أربعة'),\n",
    "                    'geem':display_arabic_text('ج'), 'gheen':display_arabic_text('غ'), 'girl_or_daughter':display_arabic_text('بنت/أبنة'),\n",
    "                    'green':display_arabic_text('أخضر'), 'haa':display_arabic_text('ح'), 'happy':display_arabic_text('سعيد'),\n",
    "                    'hehh':display_arabic_text('ه'), 'help':display_arabic_text('مساعدة'), 'home_or_house':display_arabic_text('منزل/بيت'),\n",
    "                    'hospital':display_arabic_text('مستشفى'), 'hungry':display_arabic_text('جائع'), 'ismaalia':display_arabic_text('الاسماعيلية'),\n",
    "                    'i_love_you':display_arabic_text('أحبك'), 'i_or_me':display_arabic_text('انا'), 'kaaf':display_arabic_text('ك'),\n",
    "                    'khaa':display_arabic_text('خ'), 'laam':display_arabic_text('ل'), 'luxor':display_arabic_text('الأقصر'),\n",
    "                    'meem':display_arabic_text('م'), 'monday':display_arabic_text('الأثنين'), 'nine':display_arabic_text('تسعة'),\n",
    "                    'noon':display_arabic_text('ن'), 'one':display_arabic_text('واحد'), 'orange':display_arabic_text('برتقال'),\n",
    "                    'pray':display_arabic_text('الصلاة'), 'qaaf':display_arabic_text('ق'), 'raa':display_arabic_text('ر'),\n",
    "                    'saaad':display_arabic_text('ص'), 'saturday':display_arabic_text('السبت'), 'school':display_arabic_text('مدرسة'),\n",
    "                    'seen':display_arabic_text('س'), 'seven':display_arabic_text('سبعة'), 'sharkia':display_arabic_text('الشرقية'),\n",
    "                    'sheen':display_arabic_text('ش'), 'sign':display_arabic_text('اشارة'), 'six':display_arabic_text('ستة'),\n",
    "                    'sleep_n_v':display_arabic_text('ينام/نوم'), 'son_or_boy':display_arabic_text('أبن/ولد'), 'tah':display_arabic_text('ط'),\n",
    "                    'teacher':display_arabic_text('أستاذ/معلم'), 'teen_fruit':display_arabic_text('تين'), 'teh':display_arabic_text('ت'),\n",
    "                    'ten':display_arabic_text('عشرة'), 'theh':display_arabic_text('ث'), 'the_peace':display_arabic_text('السلام'),\n",
    "                    'three':display_arabic_text('ثلاثة'), 'thursday':display_arabic_text('الخميس'), 'tuesday':display_arabic_text('الثلاثاء'),\n",
    "                    'tv':display_arabic_text('التيليفزيون'), 'two':display_arabic_text('أثنين'), 'university':display_arabic_text('الجامعة'),\n",
    "                    'waow':display_arabic_text('و'), 'water':display_arabic_text('ماء'), 'wednesday':display_arabic_text('الأربعاء'),\n",
    "                    'what':display_arabic_text('أيه/ماذا'), 'where':display_arabic_text('فين/أين'), 'white':display_arabic_text('أبيض'),\n",
    "                    'work_v':display_arabic_text('يعمل/يشتغل'), 'yeeh':display_arabic_text('ي'), 'yellow':display_arabic_text('أصفر'),\n",
    "                    'yes':display_arabic_text('نعم'), 'you':display_arabic_text('أنت'), 'your_name':display_arabic_text('أسمك'),\n",
    "                    'zaal':display_arabic_text('ذ'), 'zah':display_arabic_text('ظ'), 'zeen':display_arabic_text('ز')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "706c80a9-bd5b-456f-a1b3-7a6af4f65ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'aeen': 'ع', 'alef': 'أ', 'allah': 'الله', 'bad': 'سىء', 'bank': 'بنك', 'bathroom': 'حمام', 'beh': 'ب', 'cairo': 'القاهرة', 'chair': 'كرسي', 'child': 'طفل', 'college': 'كلية', 'daaad': 'ض', 'daal': 'د', 'deaf': 'أصم', 'dish': 'طبق', 'divorce': 'طلاق/مطلق/مطلقة', 'egypt': 'مصر', 'eight': 'ثمانية', 'engagement': 'خطوبة', 'engineer': 'مهندس', 'father': 'أب', 'feh': 'ف', 'five': 'خمسة', 'four': 'أربعة', 'geem': 'ج', 'gheen': 'غ', 'girl_or_daughter': 'بنت/أبنة', 'green': 'أخضر', 'haa': 'ح', 'happy': 'سعيد', 'hehh': 'ه', 'help': 'مساعدة', 'home_or_house': 'منزل/بيت', 'hospital': 'مستشفى', 'hungry': 'جائع', 'ismaalia': 'الاسماعيلية', 'i_love_you': 'أحبك', 'i_or_me': 'انا', 'kaaf': 'ك', 'khaa': 'خ', 'laam': 'ل', 'luxor': 'الأقصر', 'meem': 'م', 'monday': 'الأثنين', 'nine': 'تسعة', 'noon': 'ن', 'one': 'واحد', 'orange': 'برتقال', 'pray': 'الصلاة', 'qaaf': 'ق', 'raa': 'ر', 'saaad': 'ص', 'saturday': 'السبت', 'school': 'مدرسة', 'seen': 'س', 'seven': 'سبعة', 'sharkia': 'الشرقية', 'sheen': 'ش', 'sign': 'اشارة', 'six': 'ستة', 'sleep_n_v': 'ينام/نوم', 'son_or_boy': 'أبن/ولد', 'tah': 'ط', 'teacher': 'أستاذ/معلم', 'teen_fruit': 'تين', 'teh': 'ت', 'ten': 'عشرة', 'theh': 'ث', 'the_peace': 'السلام', 'three': 'ثلاثة', 'thursday': 'الخميس', 'tuesday': 'الثلاثاء', 'tv': 'التيليفزيون', 'two': 'أثنين', 'university': 'الجامعة', 'waow': 'و', 'water': 'ماء', 'wednesday': 'الأربعاء', 'what': 'أيه/ماذا', 'where': 'فين/أين', 'white': 'أبيض', 'work_v': 'يعمل/يشتغل', 'yeeh': 'ي', 'yellow': 'أصفر', 'yes': 'نعم', 'you': 'أنت', 'your_name': 'أسمك', 'zaal': 'ذ', 'zah': 'ظ', 'zeen': 'ز'}\n"
     ]
    }
   ],
   "source": [
    "new_english_to_arabic = {'aeen':'ع', 'alef':'أ', 'allah':'الله',\n",
    "                    'bad':'سىء', 'bank':'بنك', 'bathroom':'حمام',\n",
    "                    'beh':'ب', 'cairo':'القاهرة', 'chair':'كرسي',\n",
    "                    'child':'طفل', 'college':'كلية', 'daaad':'ض',\n",
    "                    'daal':'د', 'deaf':'أصم', 'dish':'طبق',\n",
    "                    'divorce':'طلاق/مطلق/مطلقة', 'egypt':'مصر', 'eight':'ثمانية',\n",
    "                    'engagement':'خطوبة', 'engineer':'مهندس', 'father':'أب',\n",
    "                    'feh':'ف', 'five':'خمسة', 'four':'أربعة',\n",
    "                    'geem':'ج', 'gheen':'غ', 'girl_or_daughter':'بنت/أبنة',\n",
    "                    'green':'أخضر', 'haa':'ح', 'happy':'سعيد',\n",
    "                    'hehh':'ه', 'help':'مساعدة', 'home_or_house':'منزل/بيت',\n",
    "                    'hospital':'مستشفى', 'hungry':'جائع', 'ismaalia':'الاسماعيلية',\n",
    "                    'i_love_you':'أحبك', 'i_or_me':'انا', 'kaaf':'ك',\n",
    "                    'khaa':'خ', 'laam':'ل', 'luxor':'الأقصر',\n",
    "                    'meem':'م', 'monday':'الأثنين', 'nine':'تسعة',\n",
    "                    'noon':'ن', 'one':'واحد', 'orange':'برتقال',\n",
    "                    'pray':'الصلاة', 'qaaf':'ق', 'raa':'ر',\n",
    "                    'saaad':'ص', 'saturday':'السبت', 'school':'مدرسة',\n",
    "                    'seen':'س', 'seven':'سبعة', 'sharkia':'الشرقية',\n",
    "                    'sheen':'ش', 'sign':'اشارة', 'six':'ستة',\n",
    "                    'sleep_n_v':'ينام/نوم', 'son_or_boy':'أبن/ولد', 'tah':'ط',\n",
    "                    'teacher':'أستاذ/معلم', 'teen_fruit':'تين', 'teh':'ت',\n",
    "                    'ten':'عشرة', 'theh':'ث', 'the_peace':'السلام',\n",
    "                    'three':'ثلاثة', 'thursday':'الخميس', 'tuesday':'الثلاثاء',\n",
    "                    'tv':'التيليفزيون', 'two':'أثنين', 'university':'الجامعة',\n",
    "                    'waow':'و', 'water':'ماء', 'wednesday':'الأربعاء',\n",
    "                    'what':'أيه/ماذا', 'where':'فين/أين', 'white':'أبيض',\n",
    "                    'work_v':'يعمل/يشتغل', 'yeeh':'ي', 'yellow':'أصفر',\n",
    "                    'yes':'نعم', 'you':'أنت', 'your_name':'أسمك',\n",
    "                    'zaal':'ذ', 'zah':'ظ', 'zeen':'ز'}\n",
    "\n",
    "print(new_english_to_arabic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938e791c-eecf-4217-8920-e0c78644f251",
   "metadata": {},
   "source": [
    "## Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c768ea2b-7529-4ad6-a740-f0a2975815c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create objects just focus on the hands \n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "# get the model that detect hand_landmarks \n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.5)\n",
    "\n",
    "# setup webcam \n",
    "camera = cv2.VideoCapture(0)\n",
    "\n",
    "# # define empty string \n",
    "# statement = str()\n",
    "\n",
    "# loop to read frames from the webcam\n",
    "while camera.isOpened():\n",
    "\n",
    "    # read frames from the webcam\n",
    "    status, frame = camera.read()\n",
    "    \n",
    "    # flip the frame \n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # convert frame to rgb\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # get hands detection on the rgb image \n",
    "    result = hands.process(frame_rgb)\n",
    "    \n",
    "    # define lists\n",
    "    current_image_landmarks = []\n",
    "    all_features = []\n",
    "    \n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmark in result.multi_hand_landmarks:\n",
    "            # draw the landmarks\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame,\n",
    "                hand_landmark, \n",
    "                mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                mp_drawing_styles.get_default_hand_connections_style())\n",
    "                \n",
    "            for landmark in hand_landmark.landmark:\n",
    "                current_image_landmarks.append(landmark.x)\n",
    "                current_image_landmarks.append(landmark.y)\n",
    "\n",
    "        # check that the number of landmarks are equal for each image\n",
    "        if len(current_image_landmarks) < 84:\n",
    "            current_image_landmarks = current_image_landmarks + [0]*(84-len(current_image_landmarks))\n",
    "\n",
    "        # append the value of current_image_data in the all_data list\n",
    "        all_features.append(current_image_landmarks)\n",
    "        \n",
    "        # convert the all_landmarks from list to 2d array\n",
    "        all_features_array = np.array(all_features)\n",
    "        \n",
    "        prediction = model.predict(all_features_array)\n",
    "        prediction_with_probability = model.predict_proba(all_features_array)\n",
    "        \n",
    "        class_label = index_to_class[prediction[0]]\n",
    "        class_label = english_to_arabic[class_label]\n",
    "        full_text = f'{class_label}, {str(prediction_with_probability[0][prediction[0]] * 100)[:4]} %'\n",
    "        \n",
    "        cv2.putText(frame, full_text, (100,100), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0,0,255), 3, cv2.LINE_AA)        \n",
    "    \n",
    "    cv2.imshow('Window', frame)\n",
    "    if cv2.waitKey(1) &  0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4a5e533e-e80a-448f-8177-a711cc5fdd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61228f7-1527-425d-bd0e-910462ab4c1b",
   "metadata": {},
   "source": [
    "## 10- Covert text to video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "a3ec276b-a49e-43f2-b776-76635a2af671",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os \n",
    "import string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "92d2f1e5-0d66-45fc-acba-99b4aa44f4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '10', '2', '3', '4', '5', '6', '7', '8', '9', 'ء', 'آ', 'أ', 'أب', 'أبن', 'أبيض', 'أثنبن', 'أحب', 'أحبك', 'أحمر', 'أخضر', 'أخوات', 'أربعة', 'أربعه', 'أرمل', 'أرملة', 'أرمله', 'أزرق', 'أسبوع', 'أسمك', 'أسمي', 'أشارة', 'أشاره', 'أصفر', 'أصم', 'أم', 'أنا', 'أنت', 'أين', 'أيه', 'ؤ', 'إ', 'إشارة', 'إشاره', 'ا', 'اب', 'ابن', 'ابيض', 'اثنين', 'احب', 'احبك', 'احمر', 'اخضر', 'اخوات', 'اربعة', 'اربعه', 'ارمل', 'ارملة', 'ارمله', 'ازرق', 'اسبوع', 'اسمك', 'اسمي', 'اشارة', 'اشاره', 'اصفر', 'اصم', 'الأبيض', 'الأثنين', 'الأحد', 'الأحمر', 'الأخضر', 'الأخوات', 'الأربعاء', 'الأزرق', 'الأسماعيلية', 'الأسماعيليه', 'الأصفر', 'الأقصر', 'الإسماعيلية', 'الإسماعيليه', 'الابيض', 'الاثنين', 'الاحد', 'الاحمر', 'الاخضر', 'الاخوات', 'الاربعاء', 'الازرق', 'الاسماعيلية', 'الاسماعيليه', 'الاصفر', 'الاقصر', 'البحر', 'البحرالأحمر', 'البحرالاحمر', 'البيت', 'الترابيزة', 'الترابيزه', 'التعارف', 'التعليم', 'التيليفيزيون', 'التين', 'الثلاثاء', 'الجمعة', 'الجمعه', 'الحمام', 'الحمد', 'الحمدلله', 'الخميس', 'الرحمة', 'الرحمه', 'السبت', 'السلام', 'الشرقية', 'الشرقيه', 'الشمال', 'الشهر', 'الصلاة', 'الصلاه', 'الطاولة', 'الطاوله', 'الطبق', 'الطفل', 'الطلاق', 'الظهر', 'العشاء', 'العصر', 'العمر', 'الفجر', 'القاهرة', 'القاهره', 'الكلية', 'الكليه', 'الله', 'الماء', 'المدرس', 'المدرسة', 'المدرسه', 'المساعدة', 'المساعده', 'المستشفى', 'المعلم', 'المغرب', 'المنزل', 'المنصورة', 'المنصوره', 'المهندس', 'الموز', 'المياة', 'النوم', 'الولد', 'اليسار', 'اليمين', 'ام', 'انا', 'انت', 'اين', 'ايه', 'ب', 'بتشتغل', 'بتعمل', 'بحب', 'بحر', 'برتقال', 'بنت', 'بنك', 'بني', 'بيت', 'ة', 'ت', 'تدرس', 'ترابيزة', 'ترابيزه', 'تسعة', 'تسعه', 'تسكن', 'تعارف', 'تعبان', 'تعليم', 'تعمل', 'تيليفيزيون', 'تين', 'ث', 'ثلاثة', 'ثلاثه', 'ثمانية', 'ثمانيه', 'ج', 'جائع', 'جامعة', 'جامعه', 'جد', 'جدة', 'جعان', 'جوعان', 'جيد', 'ح', 'حزين', 'حمام', 'حمد', 'خ', 'خطوبة', 'خطوبه', 'خمسة', 'خمسه', 'د', 'ذ', 'ر', 'رحمة', 'رحمه', 'ز', 'س', 'سئ', 'ساكن', 'سبعة', 'سبعه', 'ستة', 'سته', 'سرير', 'سعيد', 'سلام', 'سنة', 'سنه', 'سىء', 'سيء', 'ش', 'شاي', 'شبعان', 'شمال', 'شهر', 'ص', 'صفر', 'ض', 'ط', 'طاولة', 'طاوله', 'طبق', 'طفل', 'طلاق', 'ظ', 'ع', 'عشرة', 'عشره', 'عليكم', 'عندك', 'غ', 'ف', 'فين', 'ق', 'قهوة', 'قهوه', 'ك', 'كرسي', 'كلية', 'كليه', 'كوب', 'كوباية', 'كوبايه', 'ل', 'لا', 'لله', 'م', 'ماء', 'ماذا', 'متزوج', 'محتاج', 'مدرس', 'مدرسة', 'مدرسه', 'مريض', 'مساعدة', 'مساعده', 'مستشفى', 'مصر', 'مطلق', 'مطلقة', 'مطلقه', 'معلم', 'منزل', 'مهندس', 'موز', 'مياة', 'مياه', 'ن', 'نتعرف', 'نعم', 'نوم', 'ه', 'و', 'واحد', 'ورحمة', 'ورحمه', 'وعليكم', 'ولد', 'ى', 'ي', 'يأكل', 'ياكل', 'يحب', 'يدرس', 'يسار', 'يسكن', 'يشرب', 'يعمل', 'يغلق', 'يفتح', 'يمين', 'ينام', '٠', '١', '١٠', '٢', '٣', '٤', '٥', '٦', '٧', '٨', '٩']\n"
     ]
    }
   ],
   "source": [
    "# list of exist videos called mapping videos\n",
    "mapping_video_list = []\n",
    "mapping_videos = os.listdir(os.path.join(\"mapping_videos_arabic\"))\n",
    "for video in mapping_videos:\n",
    "    mapping_video_list.append(video.split(\".\")[0])\n",
    "\n",
    "print(mapping_video_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "2e7e9e5e-99b8-450f-ace7-f88829ec7964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_video_prediction_arabic(text:str):\n",
    "    \n",
    "    # convert all text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # delete any punctuation in the text\n",
    "    \n",
    "    # Create a translation table for the translate function\n",
    "    translator = str.maketrans('', '', '''!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~؟!#$\"،,.ٍِـ،/:\"><؛×÷‘ًًٌَُ$#@!%^&*)(''') \n",
    "    \n",
    "    # Remove punctuation \n",
    "    text = text.translate(translator)  \n",
    "    \n",
    "    print(text)\n",
    "    # split the whole text into separated words\n",
    "    words_list = text.split(\" \")\n",
    "    \n",
    "    # define codecc\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    \n",
    "    # define the save path \n",
    "    output_video_path = os.path.join(\"created_videos\", f\"new_created_video.avi\")\n",
    "    \n",
    "    # define video writer object to concatenate all videos in it -> by default none may be there is no words in the mapping video list\n",
    "    video_writer = None\n",
    "    \n",
    "    for word in words_list:\n",
    "        if word in mapping_video_list:\n",
    "            # means there is a video for this words \n",
    "            # access this video path \n",
    "            video_path = os.path.join(\"mapping_videos_arabic\", f\"{word}.mp4\")\n",
    "            \n",
    "            # read the video\n",
    "            video = cv2.VideoCapture(video_path)\n",
    "            \n",
    "            # extract important video features\n",
    "            width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "            height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "            fps = video.get(cv2.CAP_PROP_FPS)\n",
    "            \n",
    "            if video_writer is None:\n",
    "                video_writer = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "            \n",
    "            status = True\n",
    "            while status:\n",
    "                \n",
    "                # read Frames\n",
    "                status, frame = video.read()\n",
    "                \n",
    "                if status == True:\n",
    "                    # insert the frames of the video in the new created video [frame by frame]\n",
    "                    video_writer.write(frame)\n",
    "                \n",
    "            \n",
    "            # we get out of the first video and we will cloase it safely now\n",
    "            video.release()\n",
    "        \n",
    "        # has no corresponding words but we can express it by letters\n",
    "        else:\n",
    "            \n",
    "            # split the word to its letter and express the letters by their videos\n",
    "            word = \" \".join(word)\n",
    "            \n",
    "            for letter in word:\n",
    "                if letter == \"ة\":\n",
    "                    video_path = os.path.join(\"mapping_videos_arabic\", \"ه.mp4\")\n",
    "                else:\n",
    "                    video_path = os.path.join(\"mapping_videos_arabic\", f\"{letter}.mp4\")\n",
    "            \n",
    "                # read the video\n",
    "                video = cv2.VideoCapture(video_path)\n",
    "\n",
    "                # extract important video features\n",
    "                width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "                height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "                fps = video.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "                if video_writer is None:\n",
    "                    video_writer = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "\n",
    "                status = True\n",
    "                while status:\n",
    "\n",
    "                    # read Frames\n",
    "                    status, frame = video.read()\n",
    "\n",
    "                    if status == True:\n",
    "                        # insert the frames of the video in the new created video [frame by frame]\n",
    "                        video_writer.write(frame)\n",
    "\n",
    "                # we get out of the first video and we will cloase it safely now\n",
    "                video.release()\n",
    "            \n",
    "            \n",
    "    # here we insert all the frames of all videos in the new created video -> we will cloase this video safely \n",
    "    if video_writer != None:\n",
    "        video_writer.release()\n",
    "        return output_video_path\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a24f851-9616-40b8-9b12-58b96fbc5fb8",
   "metadata": {},
   "source": [
    "# 11- Test text to video model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "3169b558-3c86-49f9-b260-c35d7ec7cac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "انا اسمي محمد\n",
      "created_videos\\new_created_video.avi\n"
     ]
    }
   ],
   "source": [
    "print(text_to_video_prediction_arabic(\"انا اسمي محمد\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f139751c-9d7a-4670-b82b-22ccd4ab854e",
   "metadata": {},
   "source": [
    "# 12- Text to video model deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c91480-625e-42ad-a4ac-3d738f2acc5d",
   "metadata": {},
   "source": [
    "## Endpoint text to video "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "6bc391a3-9a5e-40b1-b693-9e69a4b373d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from flask import Flask, request, send_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "530df049-a944-4b89-8e41-9e3f0752cddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/text_to_video_arabic\", methods=['POST'])\n",
    "def text_to_video():\n",
    "\n",
    "    # request the text from the client as form data\n",
    "    # text_value = request.form.get('text')\n",
    "    \n",
    "    # request the text as raw data \n",
    "    text_value = str(request.data ,encoding='utf-8')\n",
    "    \n",
    "    print(type(text_value))\n",
    "    print(text_value)\n",
    "    \n",
    "\n",
    "    # apply model on the text\n",
    "    output_video_path = text_to_video_prediction_arabic(text_value)\n",
    "    print(output_video_path)\n",
    "    \n",
    "    return send_file(output_video_path)\n",
    "\n",
    "\n",
    "app.run(port=5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
