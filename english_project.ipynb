{
 "cells": [
  {
   "cell_type": "raw",
   "id": "27a7cdc7-bfc5-4c41-a365-fc003bd639ef",
   "metadata": {},
   "source": [
    "In this notebook we will make English models [video to text - text to video] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33a845f-7d2a-425c-a548-434a130d1255",
   "metadata": {},
   "source": [
    "# 1- Determine the words that we will work on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "768ea82c-591d-4367-991e-a81500682ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import os\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11aad41d-06d2-4953-899d-2e8c32b167e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'About', 'After', 'At', 'B', 'Before', 'Black', 'C', 'Can', 'Choose', 'Coffee', 'Cold', 'Congratulation', 'D', 'Doctor', 'Drink', 'E', 'Eight', 'F', 'Favourite', 'Five', 'Four', 'Friday', 'G', 'Go', 'Goodbye', 'H', 'Happy', 'Has_or_Have', 'Hearing_Aid', 'Hello', 'Help', 'How', 'I', 'I_Love_You', 'I_or_Me', 'J', 'K', 'L', 'Late', 'Live', 'Love', 'M', 'Monday', 'Month', 'My', 'My_Self', 'N', 'Name', 'Near', 'Nice', 'No', 'Now_or_Today', 'O', 'One', 'P', 'Professor', 'Q', 'R', 'Red', 'S', 'Saturday', 'Seven', 'Sit', 'Sorry', 'Stand', 'Sunday', 'T', 'Ten', 'Thank_You', 'Then', 'This', 'Three', 'Ticket', 'To', 'Tuesday', 'Two', 'U', 'V', 'W', 'Warm', 'Weather', 'Week', 'What', 'When', 'Where', 'white', 'Work', 'X', 'Y', 'Yellow', 'Yes', 'You', 'Your']\n"
     ]
    }
   ],
   "source": [
    "class_names = []\n",
    "for class_name in os.listdir(\"english\"):\n",
    "    class_names.append(class_name.split(\".\")[0])\n",
    "print(class_names) # 90 words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820e2db0-8ba2-435b-9c55-d50c320f6b04",
   "metadata": {},
   "source": [
    "## Create folder for each class in english data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a7c43b3-7cff-4898-b3eb-2d459ca35c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_name in os.listdir(\"english\"):\n",
    "    os.mkdir(os.path.join(\"english_data\", class_name.split(\".\")[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0eec737-52bc-42bc-b65d-a53d9869fa53",
   "metadata": {},
   "source": [
    "# 2- Collect images for each class [word] -> using web camera\n",
    "- we will collect 2000 images for each class and put them in folders created in english data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d20f95c2-4b48-4448-93fb-b4f3d6e162c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create objects just focus on the hands \n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "# get the model that detect hand_landmarks \n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "e4e0a219-eeb9-4ad1-ae2a-b8651bc798a5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setup my camera [my camera has the index (0)]\n",
    "my_camera = cv2.VideoCapture(0)\n",
    "\n",
    "# define a counter to give each frame a unique name \n",
    "counter = 0\n",
    "\n",
    "# In this loop, Frames will be taken from the camera until we stop it.\n",
    "while my_camera.isOpened():\n",
    "    \n",
    "    # read frame by frame from the camera -> return [frame, status of the reading process (is the camera capture the frame right or not)]\n",
    "    # status -> Boolean, Frame --> image with np.array data type\n",
    "    status, frame = my_camera.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    frame_copy = np.copy(frame)\n",
    "    \n",
    "    # convert frame to rgb\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # get hands detection on the rgb image \n",
    "    result = hands.process(frame_rgb)\n",
    "    \n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmark in result.multi_hand_landmarks:\n",
    "            # draw the landmarks\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame_copy,\n",
    "                hand_landmark, \n",
    "                mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                mp_drawing_styles.get_default_hand_connections_style())\n",
    "    \n",
    "    # display the captured frame\n",
    "    cv2.imshow('Captured Frame', frame_copy)\n",
    "\n",
    "    # save the captured frame on pressing [s]\n",
    "    # if cv2.waitKey(1) & 0xFF == ord('s'):\n",
    "    #     # increase counter by 1 \n",
    "    counter += 1 \n",
    "    cv2.imwrite(os.path.join(\"english_data\", \"Cold\", f\"new_left_Cold_frame_{counter}.jpg\"), frame)\n",
    "    # print(f'{counter}_image successfully saved')\n",
    "    \n",
    "    # Stop the reading process on pressing [q]\n",
    "    if (cv2.waitKey(1) & 0xFF == ord('q')) or counter == 2000:\n",
    "        break\n",
    "    \n",
    "my_camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bcd58a-48d6-4c92-abe7-db3d0f8d7c66",
   "metadata": {},
   "source": [
    "## Remove images that can't be detected by the google model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "35237f46-1acd-4a27-a021-2b2946edbe98",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in os.listdir(os.path.join(\"english_data\", \"Cold\")):\n",
    "    image = cv2.imread(os.path.join(\"english_data\", \"Cold\", i))\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(image_rgb)\n",
    "    \n",
    "    if result.multi_hand_landmarks:\n",
    "        continue\n",
    "    else:\n",
    "        print(i)\n",
    "        os.remove(os.path.join(\"english_data\", \"Cold\", i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98547dba-e604-41e8-9af5-d9f056d1a2af",
   "metadata": {},
   "source": [
    "## Remove Frames till 2000 frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "f4ed2522-f499-4330-95d0-eef79e97eccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "while len(os.listdir(os.path.join(\"english_data\", \"Yes\"))) > 2000:\n",
    "    random_image = random.sample(os.listdir(os.path.join(\"english_data\", \"Yes\")), k=1)\n",
    "    # print(random_image)\n",
    "    random_image_path = os.path.join(\"english_data\", \"Yes\", random_image[0])\n",
    "    os.remove(random_image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0bd64b-2117-4350-860c-969be05628e9",
   "metadata": {},
   "source": [
    "## Check that 2 hands words are detected correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f478d18-86a1-4f72-9658-c9983ee4e976",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# list of all 2 hands words \n",
    "l = [\"Sunday\", \"Has_or_Have\", \"Can\", \"Live\", \"Love\", \"Sit\", \"Doctor\", \"Stand\", \"Work\", \"What\", \"When\", \n",
    "     \"Ticket\", \"To\", \"Then\", \"This\", \"How\", \"Near\", \"Now_or_Today\", \"Professor\", \"Name\", \"Month\", \"Happy\",\n",
    "    \"Help\", \"About\", \"After\", \"At\", \"Coffee\", \"Cold\"]\n",
    "\n",
    "\n",
    "for two_hand_class_name in l: \n",
    "    # print(f\"------------------------- {two_hand_class_name} -----------------------\")\n",
    "    for i in os.listdir(os.path.join(\"english_data\", two_hand_class_name)):\n",
    "        image = cv2.imread(os.path.join(\"english_data\", two_hand_class_name, i))\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        result = hands.process(image_rgb)\n",
    "        # print(i)\n",
    "    \n",
    "        if result.multi_hand_landmarks:\n",
    "            extracted_frame_features = []\n",
    "            for hand_landmark in result.multi_hand_landmarks:\n",
    "                for landmark in hand_landmark.landmark:\n",
    "                    extracted_frame_features.append(landmark.x)\n",
    "                    extracted_frame_features.append(landmark.y)\n",
    "\n",
    "            if len(extracted_frame_features) != 84:\n",
    "                print(i)\n",
    "                print(len(extracted_frame_features))\n",
    "                os.remove(os.path.join(\"english_data\", two_hand_class_name, i))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af9e143-b771-4c7d-9e39-0bf1fa96d746",
   "metadata": {},
   "source": [
    "## Check the balancing of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8816daa-52f0-4d74-8d8a-47a9aac13aa1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for class_name in os.listdir(os.path.join('english_data')):\n",
    "    if len(os.listdir(os.path.join('english_data', class_name))) != 0:\n",
    "        print(class_name)\n",
    "        print(len(os.listdir(os.path.join('english_data', class_name))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5be931-55e2-4c95-a556-792c0c5fd508",
   "metadata": {},
   "source": [
    "# 3- Extract Features from the images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba03e6d-7c55-4135-a430-8254d3dbfd06",
   "metadata": {},
   "source": [
    "## Process : extract features and save it in .npy file for each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6427883-65da-486e-9211-bfdb4598a5bb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for class_name in os.listdir(os.path.join('english_data')):\n",
    "    if len(os.listdir(os.path.join('english_data', class_name))) != 0:\n",
    "        print(f\"------------------------------------- {class_name} ---------------------------------\")\n",
    "        for image_name in os.listdir(os.path.join('english_data', class_name)):\n",
    "            if image_name.split(\".\")[-1] == \"jpg\":\n",
    "                print(image_name)\n",
    "                # define the path of the image\n",
    "                image_path = os.path.join('english_data', class_name, image_name)\n",
    "\n",
    "                # bgr image\n",
    "                image = cv2.imread(image_path)\n",
    "\n",
    "                # rgb image \n",
    "                image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                # process the rgb image to get the hand detection\n",
    "                result = hands.process(image_rgb)\n",
    "\n",
    "                # check if there is any detection of hands or not \n",
    "                if result.multi_hand_landmarks:\n",
    "\n",
    "                    # define list to put in it x,y values for each landmarks \n",
    "                    current_image_landmarks = []\n",
    "\n",
    "                    # get x and y value for each landmark\n",
    "                    for hand_landmark in result.multi_hand_landmarks:\n",
    "                        for landmark in hand_landmark.landmark:\n",
    "                            current_image_landmarks.append(landmark.x)\n",
    "                            current_image_landmarks.append(landmark.y)\n",
    "\n",
    "                    # check that the number of landmarks are equal for each image\n",
    "                    if len(current_image_landmarks) < 84:\n",
    "                        current_image_landmarks = current_image_landmarks + [0]*(84-len(current_image_landmarks))\n",
    "\n",
    "                    # save the extracted image features in numpy array \n",
    "                    save_path = os.path.join(\"english_data\", class_name, f\"{image_name}_features\")\n",
    "                    np.save(save_path, current_image_landmarks)\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4c4b3a-d671-448d-b6ae-921cc49e600b",
   "metadata": {},
   "source": [
    "## Create label map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d47bf27-768f-4e15-a1cd-ff393894f0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "['A', 'About', 'After', 'At', 'B', 'Before', 'Black', 'C', 'Can', 'Choose', 'Coffee', 'Cold', 'D', 'Doctor', 'Drink', 'E', 'Eight', 'F', 'Favourite', 'Five', 'Four', 'Friday', 'G', 'Goodbye', 'H', 'Happy', 'Has_or_Have', 'Hearing_Aid', 'Hello', 'Help', 'How', 'I', 'I_Love_You', 'I_or_Me', 'J', 'K', 'L', 'Late', 'Live', 'Love', 'M', 'Monday', 'Month', 'My', 'My_Self', 'N', 'Name', 'Near', 'No', 'Now_or_Today', 'O', 'One', 'P', 'Professor', 'Q', 'R', 'S', 'Saturday', 'Seven', 'Sit', 'Sorry', 'Stand', 'Sunday', 'T', 'Ten', 'Thank_You', 'Then', 'This', 'Three', 'Ticket', 'To', 'Tuesday', 'Two', 'U', 'V', 'W', 'Warm', 'Weather', 'Week', 'What', 'When', 'Where', 'White', 'Work', 'X', 'Y', 'Yellow', 'Yes', 'You', 'Your']\n"
     ]
    }
   ],
   "source": [
    "classes = []\n",
    "for class_name in os.listdir(os.path.join('english_data')):\n",
    "    if len(os.listdir(os.path.join('english_data', class_name))) != 0:\n",
    "        classes.append(class_name)\n",
    "\n",
    "print(len(classes))\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13683c59-c27d-48f5-8259-f04d247499e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 0, 'About': 1, 'After': 2, 'At': 3, 'B': 4, 'Before': 5, 'Black': 6, 'C': 7, 'Can': 8, 'Choose': 9, 'Coffee': 10, 'Cold': 11, 'D': 12, 'Doctor': 13, 'Drink': 14, 'E': 15, 'Eight': 16, 'F': 17, 'Favourite': 18, 'Five': 19, 'Four': 20, 'Friday': 21, 'G': 22, 'Goodbye': 23, 'H': 24, 'Happy': 25, 'Has_or_Have': 26, 'Hearing_Aid': 27, 'Hello': 28, 'Help': 29, 'How': 30, 'I': 31, 'I_Love_You': 32, 'I_or_Me': 33, 'J': 34, 'K': 35, 'L': 36, 'Late': 37, 'Live': 38, 'Love': 39, 'M': 40, 'Monday': 41, 'Month': 42, 'My': 43, 'My_Self': 44, 'N': 45, 'Name': 46, 'Near': 47, 'No': 48, 'Now_or_Today': 49, 'O': 50, 'One': 51, 'P': 52, 'Professor': 53, 'Q': 54, 'R': 55, 'S': 56, 'Saturday': 57, 'Seven': 58, 'Sit': 59, 'Sorry': 60, 'Stand': 61, 'Sunday': 62, 'T': 63, 'Ten': 64, 'Thank_You': 65, 'Then': 66, 'This': 67, 'Three': 68, 'Ticket': 69, 'To': 70, 'Tuesday': 71, 'Two': 72, 'U': 73, 'V': 74, 'W': 75, 'Warm': 76, 'Weather': 77, 'Week': 78, 'What': 79, 'When': 80, 'Where': 81, 'White': 82, 'Work': 83, 'X': 84, 'Y': 85, 'Yellow': 86, 'Yes': 87, 'You': 88, 'Your': 89}\n"
     ]
    }
   ],
   "source": [
    "class_to_index = {class_name:index for index, class_name in enumerate(classes)}\n",
    "print(class_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2983dd-4d1b-47fe-887c-bfe7e0dd32f2",
   "metadata": {},
   "source": [
    "## Collect all the features in one array called \"all_features\" with thier labels in \"all_labels\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5eadf4b-9726-41e1-a234-b61b7521eb70",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_features = []\n",
    "all_labels = []\n",
    "\n",
    "for class_name in os.listdir(os.path.join('english_data')):\n",
    "    print(f\"-------------------------------- {class_name} -----------------------------\")\n",
    "    for numpy_file in os.listdir(os.path.join('english_data', class_name)):\n",
    "        if numpy_file.split(\".\")[-1] == \"npy\":\n",
    "            print(numpy_file)\n",
    "            # load the numpy file \n",
    "            numpy_file_path = os.path.join('english_data', class_name, numpy_file)\n",
    "            numpy_array = np.load(numpy_file_path)\n",
    "            all_features.append(numpy_array)\n",
    "            all_labels.append(class_to_index[class_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfcb205-e237-4317-9206-c66a78019786",
   "metadata": {},
   "source": [
    "## Save all_features and all_labels lists to external file called \"english_data.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fdc4ec9e-d170-4dbe-a9d4-0bb62548ae26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all_landmarks and labels in file called data.pickle using pickle module \n",
    "data_file = open('english_data.pickle','wb')\n",
    "pickle.dump({'all_features':all_features, 'all_labels':all_labels}, data_file)\n",
    "data_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dc8e41-05c9-4001-8a64-b881fe4f50e5",
   "metadata": {},
   "source": [
    "## Load all_features and all_labels to use them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "174cc12b-e773-4b26-9ae2-3caaced03b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the values from the file \n",
    "data_file = open('english_data.pickle', 'rb')\n",
    "data = pickle.load(data_file)\n",
    "\n",
    "all_features = data['all_features']\n",
    "all_labels = data['all_labels']\n",
    "\n",
    "data_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967d044e-aaac-40e5-af3f-9149ae0d3a17",
   "metadata": {},
   "source": [
    "## Convert all_features and all_labels to arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd042baa-8f8c-4f5c-910b-e3eebd1f54d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(180000, 84)\n",
      "(180000,)\n"
     ]
    }
   ],
   "source": [
    "# convert the data to arrays \n",
    "all_features_array = np.array(all_features)\n",
    "all_labels_array = np.array(all_labels)\n",
    "\n",
    "print(type(all_features_array))\n",
    "print(type(all_labels_array))\n",
    "\n",
    "print(all_features_array.shape)\n",
    "print(all_labels_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1947b79f-da7b-4527-97d0-30107890ee57",
   "metadata": {},
   "source": [
    "# 4- Dataset Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "06dda460-0706-4c6a-a665-836d71aafb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "afe8c151-33fd-440a-9fab-bac0d9721b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(all_features_array, all_labels_array, test_size=0.2, shuffle=True, stratify=all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b747331d-262d-4dc1-81f7-c97076a1d884",
   "metadata": {},
   "source": [
    "# 5- Model Building "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f0eff80b-1fb9-4b43-ba9d-5c8016b210d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c9b1a7cb-3e3f-4224-a176-51890b5f2db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_random_forest = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3ea5e87e-38f6-43d4-9ba7-cf35790afb0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;RandomForestClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">?<span>Documentation for RandomForestClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_random_forest.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd293369-f0c4-45a9-8e20-d7741dc2aea1",
   "metadata": {},
   "source": [
    "## Save the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b3f462d5-7b87-476e-a55a-5d7051dee904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "model_file = open('english_model.pickle','wb')\n",
    "pickle.dump({'model':model_random_forest}, model_file)\n",
    "model_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0864fff4-41bc-4040-a694-08ed3093e577",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2285ce4b-6c7a-40c6-abe9-d98e12bb445a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model_file = open('english_model.pickle','rb')\n",
    "model_dict = pickle.load(model_file)\n",
    "model = model_dict['model']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b723053-09bb-48bc-93a5-9c265e81c32f",
   "metadata": {},
   "source": [
    "# 6-Model Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bb11ee67-90cf-4705-a674-522c59713de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e20f91-d34c-485c-90f6-78dde7f30cf6",
   "metadata": {},
   "source": [
    "## Calculate y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6588314-d735-487a-a4e1-313c5547a3ff",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the y_predict \n",
    "y_predict = model.predict(x_test)\n",
    "print(y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58962e6e-acb2-41f8-929b-463dc14743dd",
   "metadata": {},
   "source": [
    "## Calculate confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d516d33e-1ccb-48bd-b429-f53b78105d32",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_predict)\n",
    "for i in cm:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a9186f-ee55-4be9-beb9-6fca212b7b5f",
   "metadata": {},
   "source": [
    "## Calculate Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3e6fb47-001d-4db1-ae62-5b4f8a4a353b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9986388888888889\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_predict)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ae2eb6-ec0a-490f-8294-529ba5511742",
   "metadata": {},
   "source": [
    "## Calculate Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24837c1c-6706-419e-b7bf-934388ffd962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9986388888888889\n"
     ]
    }
   ],
   "source": [
    "precision = precision_score(y_test, y_predict, average='micro')\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818461cd-7d89-499b-8aba-4b743edcdda6",
   "metadata": {},
   "source": [
    "## Calculate F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2aa4fac7-eda2-4b4f-98ea-88c93916a75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9986388888888889\n"
     ]
    }
   ],
   "source": [
    "f1 = f1_score(y_test, y_predict, average='micro')\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89b45b9-1e38-49d7-9a27-049f1b631fa3",
   "metadata": {},
   "source": [
    "## Calculate Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3f60d70-2c7f-4877-8da5-41d8a03e82a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9986388888888889\n"
     ]
    }
   ],
   "source": [
    "recall = recall_score(y_test, y_predict, average='micro')\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801d7a93-64cf-4f71-9931-04891385eecd",
   "metadata": {},
   "source": [
    "## Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c041a98b-4686-40e5-ad47-1e31055a8cde",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       400\n",
      "           1       1.00      1.00      1.00       400\n",
      "           2       1.00      1.00      1.00       400\n",
      "           3       1.00      1.00      1.00       400\n",
      "           4       1.00      1.00      1.00       400\n",
      "           5       1.00      1.00      1.00       400\n",
      "           6       1.00      1.00      1.00       400\n",
      "           7       1.00      1.00      1.00       400\n",
      "           8       1.00      1.00      1.00       400\n",
      "           9       1.00      1.00      1.00       400\n",
      "          10       1.00      1.00      1.00       400\n",
      "          11       1.00      1.00      1.00       400\n",
      "          12       1.00      1.00      1.00       400\n",
      "          13       1.00      1.00      1.00       400\n",
      "          14       1.00      1.00      1.00       400\n",
      "          15       1.00      0.99      1.00       400\n",
      "          16       1.00      1.00      1.00       400\n",
      "          17       1.00      1.00      1.00       400\n",
      "          18       1.00      1.00      1.00       400\n",
      "          19       1.00      1.00      1.00       400\n",
      "          20       1.00      1.00      1.00       400\n",
      "          21       1.00      1.00      1.00       400\n",
      "          22       1.00      1.00      1.00       400\n",
      "          23       1.00      1.00      1.00       400\n",
      "          24       1.00      1.00      1.00       400\n",
      "          25       1.00      1.00      1.00       400\n",
      "          26       1.00      1.00      1.00       400\n",
      "          27       1.00      1.00      1.00       400\n",
      "          28       1.00      0.99      1.00       400\n",
      "          29       1.00      1.00      1.00       400\n",
      "          30       1.00      0.99      1.00       400\n",
      "          31       1.00      1.00      1.00       400\n",
      "          32       1.00      1.00      1.00       400\n",
      "          33       1.00      1.00      1.00       400\n",
      "          34       1.00      1.00      1.00       400\n",
      "          35       1.00      1.00      1.00       400\n",
      "          36       1.00      0.99      1.00       400\n",
      "          37       0.99      1.00      1.00       400\n",
      "          38       1.00      1.00      1.00       400\n",
      "          39       1.00      1.00      1.00       400\n",
      "          40       1.00      1.00      1.00       400\n",
      "          41       1.00      1.00      1.00       400\n",
      "          42       1.00      1.00      1.00       400\n",
      "          43       1.00      1.00      1.00       400\n",
      "          44       1.00      1.00      1.00       400\n",
      "          45       1.00      1.00      1.00       400\n",
      "          46       1.00      1.00      1.00       400\n",
      "          47       1.00      1.00      1.00       400\n",
      "          48       1.00      1.00      1.00       400\n",
      "          49       1.00      1.00      1.00       400\n",
      "          50       0.99      1.00      1.00       400\n",
      "          51       1.00      1.00      1.00       400\n",
      "          52       1.00      0.99      0.99       400\n",
      "          53       1.00      1.00      1.00       400\n",
      "          54       1.00      0.99      1.00       400\n",
      "          55       0.98      0.98      0.98       400\n",
      "          56       1.00      0.99      0.99       400\n",
      "          57       1.00      1.00      1.00       400\n",
      "          58       1.00      1.00      1.00       400\n",
      "          59       1.00      1.00      1.00       400\n",
      "          60       1.00      1.00      1.00       400\n",
      "          61       1.00      1.00      1.00       400\n",
      "          62       1.00      1.00      1.00       400\n",
      "          63       1.00      1.00      1.00       400\n",
      "          64       1.00      1.00      1.00       400\n",
      "          65       1.00      1.00      1.00       400\n",
      "          66       0.99      1.00      1.00       400\n",
      "          67       1.00      1.00      1.00       400\n",
      "          68       1.00      1.00      1.00       400\n",
      "          69       1.00      1.00      1.00       400\n",
      "          70       1.00      1.00      1.00       400\n",
      "          71       1.00      1.00      1.00       400\n",
      "          72       1.00      1.00      1.00       400\n",
      "          73       0.98      0.98      0.98       400\n",
      "          74       1.00      0.99      1.00       400\n",
      "          75       0.99      1.00      1.00       400\n",
      "          76       1.00      1.00      1.00       400\n",
      "          77       1.00      0.99      0.99       400\n",
      "          78       1.00      1.00      1.00       400\n",
      "          79       1.00      1.00      1.00       400\n",
      "          80       1.00      1.00      1.00       400\n",
      "          81       1.00      1.00      1.00       400\n",
      "          82       1.00      1.00      1.00       400\n",
      "          83       1.00      1.00      1.00       400\n",
      "          84       1.00      1.00      1.00       400\n",
      "          85       1.00      1.00      1.00       400\n",
      "          86       1.00      1.00      1.00       400\n",
      "          87       1.00      1.00      1.00       400\n",
      "          88       1.00      1.00      1.00       400\n",
      "          89       1.00      1.00      1.00       400\n",
      "\n",
      "    accuracy                           1.00     36000\n",
      "   macro avg       1.00      1.00      1.00     36000\n",
      "weighted avg       1.00      1.00      1.00     36000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, y_predict)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f15bbf8-7ee1-41fc-8a5a-ca9ea2d68531",
   "metadata": {},
   "source": [
    "# 7- Test model on videos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68f25c3-11a4-4221-a602-4022c2b5f7e6",
   "metadata": {},
   "source": [
    "## Load google model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10c2ea87-7ecd-4637-8042-a29ade23626a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create object just focus on the hands \n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# get the model that detect hand_landmarks \n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b260534-1804-45af-919b-cbbd542f2626",
   "metadata": {},
   "source": [
    "## Load my model model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65c6e89d-2578-48c4-af23-2df5f4a6dc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model_file = open('english_model.pickle','rb')\n",
    "model_dict = pickle.load(model_file)\n",
    "model = model_dict['model']\n",
    "model_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2cdc96-6171-40ab-819d-10dafd385231",
   "metadata": {},
   "source": [
    "## Create label map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a579de60-ff24-4b9e-8208-3abc6a67139b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'About', 'After', 'At', 'B', 'Before', 'Black', 'C', 'Can', 'Choose', 'Coffee', 'Cold', 'D', 'Doctor', 'Drink', 'E', 'Eight', 'F', 'Favorite', 'Five', 'Four', 'Friday', 'G', 'Goodbye', 'H', 'Happy', 'Has_or_Have', 'Hearing_Aid', 'Hello', 'Help', 'How', 'I', 'I_Love_You', 'I_or_Me', 'J', 'K', 'L', 'Late', 'Live', 'Love', 'M', 'Monday', 'Month', 'My', 'My_Self', 'N', 'Name', 'Near', 'No', 'Now_or_Today', 'O', 'One', 'P', 'Professor', 'Q', 'R', 'S', 'Saturday', 'Seven', 'Sit', 'Sorry', 'Stand', 'Sunday', 'T', 'Ten', 'Thank_You', 'Then', 'This', 'Three', 'Ticket', 'To', 'Tuesday', 'Two', 'U', 'V', 'W', 'Warm', 'Weather', 'Week', 'What', 'When', 'Where', 'White', 'Work', 'X', 'Y', 'Yellow', 'Yes', 'You', 'Your']\n"
     ]
    }
   ],
   "source": [
    "classes = []\n",
    "for class_name in os.listdir(os.path.join('english_data')):\n",
    "    if len(os.listdir(os.path.join('english_data', class_name))) != 0:\n",
    "        classes.append(class_name)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "736a6916-8a8b-4f76-b19e-9a911b5928ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'A', 1: 'About', 2: 'After', 3: 'At', 4: 'B', 5: 'Before', 6: 'Black', 7: 'C', 8: 'Can', 9: 'Choose', 10: 'Coffee', 11: 'Cold', 12: 'D', 13: 'Doctor', 14: 'Drink', 15: 'E', 16: 'Eight', 17: 'F', 18: 'Favorite', 19: 'Five', 20: 'Four', 21: 'Friday', 22: 'G', 23: 'Goodbye', 24: 'H', 25: 'Happy', 26: 'Has_or_Have', 27: 'Hearing_Aid', 28: 'Hello', 29: 'Help', 30: 'How', 31: 'I', 32: 'I_Love_You', 33: 'I_or_Me', 34: 'J', 35: 'K', 36: 'L', 37: 'Late', 38: 'Live', 39: 'Love', 40: 'M', 41: 'Monday', 42: 'Month', 43: 'My', 44: 'My_Self', 45: 'N', 46: 'Name', 47: 'Near', 48: 'No', 49: 'Now_or_Today', 50: 'O', 51: 'One', 52: 'P', 53: 'Professor', 54: 'Q', 55: 'R', 56: 'S', 57: 'Saturday', 58: 'Seven', 59: 'Sit', 60: 'Sorry', 61: 'Stand', 62: 'Sunday', 63: 'T', 64: 'Ten', 65: 'Thank_You', 66: 'Then', 67: 'This', 68: 'Three', 69: 'Ticket', 70: 'To', 71: 'Tuesday', 72: 'Two', 73: 'U', 74: 'V', 75: 'W', 76: 'Warm', 77: 'Weather', 78: 'Week', 79: 'What', 80: 'When', 81: 'Where', 82: 'White', 83: 'Work', 84: 'X', 85: 'Y', 86: 'Yellow', 87: 'Yes', 88: 'You', 89: 'Your'}\n"
     ]
    }
   ],
   "source": [
    "index_to_class = {index:class_name for index, class_name in enumerate(classes)}\n",
    "print(index_to_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa3610f-0e8d-4c4b-8d50-7e159b6458ef",
   "metadata": {},
   "source": [
    "## Function: take image and output label and its probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0578af2-71f5-4173-b1d4-a183d6a2e1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define method to predict image with the random forest model\n",
    "# image -> label, probablity of it \n",
    "\n",
    "def model_image_predict(image):\n",
    "    \n",
    "    # convert image to rgb \n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # get landmarks\n",
    "    result = hands.process(image_rgb)\n",
    "    \n",
    "    # define list to put all the landmarks in it \n",
    "    all_features = []\n",
    "    \n",
    "    # define list to put in it x,y values for each landmarks \n",
    "    current_image_landmarks = []\n",
    "\n",
    "    # get x and y value for each landmark\n",
    "\n",
    "    # check if there is any detection of hands or not \n",
    "    if result.multi_hand_landmarks:\n",
    "\n",
    "        for hand_landmark in result.multi_hand_landmarks:\n",
    "            for landmark in hand_landmark.landmark:\n",
    "                current_image_landmarks.append(landmark.x)\n",
    "                current_image_landmarks.append(landmark.y)\n",
    "\n",
    "        # check that the number of landmarks are equal for each image\n",
    "        if len(current_image_landmarks) < 84:\n",
    "            current_image_landmarks = current_image_landmarks + [0]*(84-len(current_image_landmarks))\n",
    "\n",
    "        # append the value of current_image_data in the all_data list\n",
    "        all_features.append(current_image_landmarks)\n",
    "        \n",
    "        # convert the all_landmarks from list to 2d array\n",
    "        all_features_array = np.array(all_features)\n",
    "        \n",
    "        \n",
    "        prediction = model.predict(all_features_array)\n",
    "        prediction_with_probability = model.predict_proba(all_features_array)\n",
    "\n",
    "        return {'class':prediction[0], 'probability':prediction_with_probability[0][prediction[0]]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541f7217-6d5d-43c2-8b6e-b53097c603af",
   "metadata": {},
   "source": [
    "## Test model prediction on images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23c23f10-f539-4eb9-97e0-8743ec41589f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class': 0, 'probability': 0.98}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "image_path = os.path.join(\"english_data\", \"A\", \"new_left_A_frame_1.jpg\")\n",
    "image = cv2.imread(image_path)\n",
    "prediction = model_image_predict(image)\n",
    "print(prediction)\n",
    "print(type(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33ac797-b40a-4996-8681-850ca923df39",
   "metadata": {},
   "source": [
    "## Function: Convert the video to text\n",
    "Be attention for 2 important things :\n",
    "- Frame Per Second [FPS]\n",
    "- Video Status: \n",
    "    - 1 sign language \n",
    "    - Multiple sign language "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbcc7b14-987b-4efe-9d60-3cede8a13ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_to_text_prediction(video_path):\n",
    "\n",
    "    # read the video\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # calculate the frame per second of the video -> take 1 frame per 1/2 second\n",
    "    fps = round(round(video.get(cv2.CAP_PROP_FPS))/2) # 30 frmas per sec [in each second we will get 2 frames for detection process]\n",
    "\n",
    "    # define frame_counter variable --> when it reach 15 we will take this frame [so we take a frame after 1/2 second]\n",
    "    frame_counter = 0\n",
    "\n",
    "    # get all predictions from the video in all_prediction dictionary \n",
    "    all_predictions = dict()\n",
    "    prediction_id = 0\n",
    "\n",
    "    status = True  \n",
    "    while status:\n",
    "        # read frames from the video\n",
    "        status, frame = video.read()\n",
    "\n",
    "        if status == True:\n",
    "            # count the current frame\n",
    "            frame_counter += 1\n",
    "            # here is the frame that will be used for prediction\n",
    "            if frame_counter % fps == 0:\n",
    "                prediction = model_image_predict(frame)\n",
    "                prediction_id += 1 \n",
    "                all_predictions[prediction_id] = prediction\n",
    "    \n",
    "    \n",
    "    threshold = 0.25\n",
    "    all_classes = str()\n",
    "    for i in all_predictions:\n",
    "        # check if there is a prediction or not\n",
    "        if all_predictions[i]:\n",
    "            if all_predictions[i]['probability'] > threshold:\n",
    "                all_classes += index_to_class[all_predictions[i]['class']] + ' '\n",
    "    # make unique classes [Cancel repetition]\n",
    "    previous_class = None\n",
    "    final_classes = []\n",
    "\n",
    "    for i in all_classes.rstrip().split():\n",
    "\n",
    "        if i != previous_class:\n",
    "            final_classes.append(i)\n",
    "            previous_class = i\n",
    "\n",
    "    return \" \".join(final_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebcf916-3781-4933-bd1f-102b965c8724",
   "metadata": {},
   "source": [
    "## Test model predicion on videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "519370f2-d4e7-4ebd-b582-e49e9e6b8637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I_Love_You\n"
     ]
    }
   ],
   "source": [
    "print(video_to_text_prediction(os.path.join(\"english_test_videos\", \"i_love_you.mp4\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cb19b3-8415-45ff-82a9-a9682cdc5dbb",
   "metadata": {},
   "source": [
    "# 8- Video to text model deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9554b9d-2e35-4d61-8111-27abf31db912",
   "metadata": {},
   "source": [
    "## Endpoint video to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "526561a8-d103-4fe3-b542-704a533b9b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from flask import Flask, request, jsonify\n",
    "from celery import Celery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c2be6a1-03a2-42c5-a025-29d46fbc5694",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/video_to_text_english\", methods=['POST'])\n",
    "def video_to_text():\n",
    "\n",
    "    # upload file\n",
    "    # this is the video itself\n",
    "    video = request.files['video']\n",
    "    file_name = video.filename.split(\"/\")[-1]\n",
    "    \n",
    "    print(\"this is the path of the video\", file_name)\n",
    "\n",
    "    # save the video -> to access it and make the detection process\n",
    "    # after saving the video we will get the 1st parameter for the detection method -> [video_path]\n",
    "    video.save(os.path.join('uploaded', file_name))\n",
    "\n",
    "    # apply model on it\n",
    "    output = video_to_text_prediction(os.path.join('uploaded', file_name))\n",
    "    \n",
    "    return jsonify([{\"text\":output}])\n",
    "\n",
    "\n",
    "app.run(port=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a11e85-a78a-444d-89b4-09c7efe19d0b",
   "metadata": {},
   "source": [
    "# 9- Test in real time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e1665dfb-b9b9-44d4-ae4e-c3749a7e512c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# load model \n",
    "model_file = open('english_model.pickle','rb')\n",
    "model_dict = pickle.load(model_file)\n",
    "model = model_dict['model']\n",
    "model_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b9fa246a-d60c-4b28-b937-d58d1c49262e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'About', 'After', 'At', 'B', 'Before', 'Black', 'C', 'Can', 'Choose', 'Coffee', 'Cold', 'D', 'Doctor', 'Drink', 'E', 'Eight', 'F', 'Favourite', 'Five', 'Four', 'Friday', 'G', 'Goodbye', 'H', 'Happy', 'Has_or_Have', 'Hearing_Aid', 'Hello', 'Help', 'How', 'I', 'I_Love_You', 'I_or_Me', 'J', 'K', 'L', 'Late', 'Live', 'Love', 'M', 'Monday', 'Month', 'My', 'My_Self', 'N', 'Name', 'Near', 'No', 'Now_or_Today', 'O', 'One', 'P', 'Professor', 'Q', 'R', 'S', 'Saturday', 'Seven', 'Sit', 'Sorry', 'Stand', 'Sunday', 'T', 'Ten', 'Thank_You', 'Then', 'This', 'Three', 'Ticket', 'To', 'Tuesday', 'Two', 'U', 'V', 'W', 'Warm', 'Weather', 'Week', 'What', 'When', 'Where', 'White', 'Work', 'X', 'Y', 'Yellow', 'Yes', 'You', 'Your']\n"
     ]
    }
   ],
   "source": [
    "classes = []\n",
    "for class_name in os.listdir(os.path.join('english_data')):\n",
    "    if len(os.listdir(os.path.join('english_data', class_name))) != 0:\n",
    "        classes.append(class_name)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "82286f45-b8ed-400a-a5d4-209c51445631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'A', 1: 'About', 2: 'After', 3: 'At', 4: 'B', 5: 'Before', 6: 'Black', 7: 'C', 8: 'Can', 9: 'Choose', 10: 'Coffee', 11: 'Cold', 12: 'D', 13: 'Doctor', 14: 'Drink', 15: 'E', 16: 'Eight', 17: 'F', 18: 'Favourite', 19: 'Five', 20: 'Four', 21: 'Friday', 22: 'G', 23: 'Goodbye', 24: 'H', 25: 'Happy', 26: 'Has_or_Have', 27: 'Hearing_Aid', 28: 'Hello', 29: 'Help', 30: 'How', 31: 'I', 32: 'I_Love_You', 33: 'I_or_Me', 34: 'J', 35: 'K', 36: 'L', 37: 'Late', 38: 'Live', 39: 'Love', 40: 'M', 41: 'Monday', 42: 'Month', 43: 'My', 44: 'My_Self', 45: 'N', 46: 'Name', 47: 'Near', 48: 'No', 49: 'Now_or_Today', 50: 'O', 51: 'One', 52: 'P', 53: 'Professor', 54: 'Q', 55: 'R', 56: 'S', 57: 'Saturday', 58: 'Seven', 59: 'Sit', 60: 'Sorry', 61: 'Stand', 62: 'Sunday', 63: 'T', 64: 'Ten', 65: 'Thank_You', 66: 'Then', 67: 'This', 68: 'Three', 69: 'Ticket', 70: 'To', 71: 'Tuesday', 72: 'Two', 73: 'U', 74: 'V', 75: 'W', 76: 'Warm', 77: 'Weather', 78: 'Week', 79: 'What', 80: 'When', 81: 'Where', 82: 'White', 83: 'Work', 84: 'X', 85: 'Y', 86: 'Yellow', 87: 'Yes', 88: 'You', 89: 'Your'}\n"
     ]
    }
   ],
   "source": [
    "index_to_class = {index:class_name for index, class_name in enumerate(classes)}\n",
    "print(index_to_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938e791c-eecf-4217-8920-e0c78644f251",
   "metadata": {},
   "source": [
    "## Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c768ea2b-7529-4ad6-a740-f0a2975815c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create objects just focus on the hands \n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "# get the model that detect hand_landmarks \n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.5)\n",
    "\n",
    "# setup webcam \n",
    "camera = cv2.VideoCapture(0)\n",
    "\n",
    "# # define empty string \n",
    "# statement = str()\n",
    "\n",
    "# loop to read frames from the webcam\n",
    "while camera.isOpened():\n",
    "\n",
    "    # read frames from the webcam\n",
    "    status, frame = camera.read()\n",
    "    \n",
    "    # flip the frame \n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # convert frame to rgb\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # get hands detection on the rgb image \n",
    "    result = hands.process(frame_rgb)\n",
    "    \n",
    "    # define lists\n",
    "    current_image_landmarks = []\n",
    "    all_features = []\n",
    "    \n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmark in result.multi_hand_landmarks:\n",
    "            # draw the landmarks\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame,\n",
    "                hand_landmark, \n",
    "                mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                mp_drawing_styles.get_default_hand_connections_style())\n",
    "                \n",
    "            for landmark in hand_landmark.landmark:\n",
    "                current_image_landmarks.append(landmark.x)\n",
    "                current_image_landmarks.append(landmark.y)\n",
    "\n",
    "        # check that the number of landmarks are equal for each image\n",
    "        if len(current_image_landmarks) < 84:\n",
    "            current_image_landmarks = current_image_landmarks + [0]*(84-len(current_image_landmarks))\n",
    "\n",
    "        # append the value of current_image_data in the all_data list\n",
    "        all_features.append(current_image_landmarks)\n",
    "        \n",
    "        # convert the all_landmarks from list to 2d array\n",
    "        all_features_array = np.array(all_features)\n",
    "        \n",
    "        prediction = model.predict(all_features_array)\n",
    "        prediction_with_probability = model.predict_proba(all_features_array)\n",
    "        \n",
    "        class_label = index_to_class[prediction[0]]\n",
    "        full_text = f'{class_label}, {str(prediction_with_probability[0][prediction[0]] * 100)[:4]} %'\n",
    "        \n",
    "        cv2.putText(frame, full_text, (100,100), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0,0,255), 3, cv2.LINE_AA)        \n",
    "    \n",
    "    cv2.imshow('Window', frame)\n",
    "    if cv2.waitKey(1) &  0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4a5e533e-e80a-448f-8177-a711cc5fdd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61228f7-1527-425d-bd0e-910462ab4c1b",
   "metadata": {},
   "source": [
    "## 10- Covert text to video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3ec276b-a49e-43f2-b776-76635a2af671",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os \n",
    "import string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92d2f1e5-0d66-45fc-acba-99b4aa44f4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '10', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'about', 'after', 'at', 'b', 'before', 'black', 'c', 'can', 'choose', 'coffee', 'cold', 'd', 'doctor', 'drink', 'e', 'eight', 'f', 'favorite', 'five', 'four', 'friday', 'g', 'goodbye', 'h', 'happy', 'has', 'have', 'hearing_aid', 'hello', 'help', 'how', 'i', 'i_love_you', 'j', 'k', 'l', 'late', 'live', 'love', 'm', 'me', 'monday', 'month', 'my', 'myself', 'n', 'name', 'near', 'nine', 'no', 'now', 'o', 'one', 'p', 'professor', 'q', 'r', 's', 'saturday', 'seven', 'sit', 'six', 'sorry', 'stand', 'sunday', 't', 'ten', 'thank_you', 'then', 'this', 'three', 'ticket', 'to', 'today', 'tuesday', 'u', 'v', 'w', 'warm', 'weather', 'week', 'what', 'when', 'where', 'white', 'work', 'x', 'y', 'yellow', 'yes', 'you', 'your', 'z', 'zero']\n"
     ]
    }
   ],
   "source": [
    "# list of exist videos called mapping videos\n",
    "mapping_video_list = []\n",
    "mapping_videos = os.listdir(os.path.join(\"mapping_videos_english\"))\n",
    "for video in mapping_videos:\n",
    "    mapping_video_list.append(video.split(\".\")[0])\n",
    "\n",
    "print(mapping_video_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e7e9e5e-99b8-450f-ace7-f88829ec7964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_video_prediction(text:str):\n",
    "    \n",
    "    # convert all text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # delete any punctuation in the text\n",
    "    \n",
    "    # Create a translation table for the translate function\n",
    "    translator = str.maketrans('', '', string.punctuation) \n",
    "    \n",
    "    # Remove punctuation \n",
    "    text = text.translate(translator)  \n",
    "    \n",
    "    # check the multi-words\n",
    "    if 'thank you' in text:\n",
    "        text = text.replace('thank you', 'thank_you')\n",
    "    \n",
    "    if 'hearing aid' in text:\n",
    "        text = text.replace('hearing aid', 'hearing_aid')\n",
    "        \n",
    "    if 'i love you' in text:\n",
    "        text = text.replace('i love you', 'i_love_you')\n",
    "    \n",
    "    print(text)\n",
    "    # split the whole text into separated words\n",
    "    words_list = text.split(\" \")\n",
    "    \n",
    "    # define codecc\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    \n",
    "    # define the save path \n",
    "    output_video_path = os.path.join(\"created_videos\", \"new_created_video.avi\")\n",
    "    \n",
    "    # define video writer object to concatenate all videos in it -> by default none may be there is no words in the mapping video list\n",
    "    video_writer = None\n",
    "    \n",
    "    for word in words_list:\n",
    "        if word in mapping_video_list:\n",
    "            # means there is a video for this words \n",
    "            # access this video path \n",
    "            video_path = os.path.join(\"mapping_videos_english\", f\"{word}.mp4\")\n",
    "            \n",
    "            # read the video\n",
    "            video = cv2.VideoCapture(video_path)\n",
    "            \n",
    "            # extract important video features\n",
    "            width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "            height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "            fps = video.get(cv2.CAP_PROP_FPS)\n",
    "            \n",
    "            if video_writer is None:\n",
    "                video_writer = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "            \n",
    "            status = True\n",
    "            while status:\n",
    "                \n",
    "                # read Frames\n",
    "                status, frame = video.read()\n",
    "                \n",
    "                if status == True:\n",
    "                    # insert the frames of the video in the new created video [frame by frame]\n",
    "                    video_writer.write(frame)\n",
    "                \n",
    "            \n",
    "            # we get out of the first video and we will cloase it safely now\n",
    "            video.release()\n",
    "        \n",
    "        # has no corresponding words but we can express it by letters\n",
    "        else:\n",
    "            \n",
    "            # split the word to its letter and express the letters by their videos\n",
    "            word = \" \".join(word)\n",
    "            \n",
    "            for letter in word:\n",
    "                video_path = os.path.join(\"mapping_videos_english\", f\"{letter}.mp4\")\n",
    "            \n",
    "                # read the video\n",
    "                video = cv2.VideoCapture(video_path)\n",
    "\n",
    "                # extract important video features\n",
    "                width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "                height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "                fps = video.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "                if video_writer is None:\n",
    "                    video_writer = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "\n",
    "                status = True\n",
    "                while status:\n",
    "\n",
    "                    # read Frames\n",
    "                    status, frame = video.read()\n",
    "\n",
    "                    if status == True:\n",
    "                        # insert the frames of the video in the new created video [frame by frame]\n",
    "                        video_writer.write(frame)\n",
    "\n",
    "                # we get out of the first video and we will cloase it safely now\n",
    "                video.release()\n",
    "            \n",
    "            \n",
    "    # here we insert all the frames of all videos in the new created video -> we will cloase this video safely \n",
    "    if video_writer != None:\n",
    "        video_writer.release()\n",
    "        return output_video_path\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d6b32d-ca44-4a3e-b5d8-7d1b544b32e1",
   "metadata": {},
   "source": [
    "# 11- Test text to video model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3169b558-3c86-49f9-b260-c35d7ec7cac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my name is mohamed iam 22 years old i love python what about you\n",
      "created_videos\\new_created_video.avi\n"
     ]
    }
   ],
   "source": [
    "print(text_to_video_prediction('My Name Is Mohamed, Iam 22 Years old. I Love Python!!!,#$%@^^ what about you?!@'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c597350f-6eaf-4df1-9e54-51fc167d3dbe",
   "metadata": {},
   "source": [
    "# 12- Text to video model deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c91480-625e-42ad-a4ac-3d738f2acc5d",
   "metadata": {},
   "source": [
    "## Endpoint text to video "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bc391a3-9a5e-40b1-b693-9e69a4b373d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from flask import Flask, request, send_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "530df049-a944-4b89-8e41-9e3f0752cddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/text_to_video_english\", methods=['POST'])\n",
    "def text_to_video():\n",
    "\n",
    "    # request the text from the client as form data\n",
    "    # text_value = request.form.get('text')\n",
    "    \n",
    "    # request the text as raw data \n",
    "    text_value = str(request.data ,encoding='utf-8')\n",
    "    \n",
    "    print(type(text_value))\n",
    "    print(text_value)\n",
    "    \n",
    "\n",
    "    # apply model on the text\n",
    "    output_video_path = text_to_video_prediction(text_value)\n",
    "    print(output_video_path)\n",
    "    \n",
    "    return send_file(output_video_path)\n",
    "\n",
    "\n",
    "app.run(port=5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
